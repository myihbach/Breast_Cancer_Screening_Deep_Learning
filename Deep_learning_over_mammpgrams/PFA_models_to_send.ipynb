{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "PFA_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9eYwQ9tzO2i"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H-5mclj5d4Y"
      },
      "source": [
        "# import the architecture\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCcBsxOTQOqs",
        "outputId": "2425b92a-d111-4b0c-dbb1-4a40e2ab5444"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZogLGTiS6a_",
        "outputId": "a137629a-1b8b-4b45-ef70-c4b1fb7de929"
      },
      "source": [
        "%cd /content/drive/MyDrive/'Colab Notebooks'/\n",
        "# import our created package\n",
        "import training_pg as pg\n",
        "import importlib\n",
        "# reloading the package if we make any change on it\n",
        "importlib.reload(pg)\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24PoHMbDIuPh"
      },
      "source": [
        "## **Why this Block?**\n",
        "To work on other models or on other Dataset i just copy this notebook and i modify  these variables (modifying ones). So this way I gain time and avoid making errors (forgetting to change in other places)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGsn8l9W5d7u"
      },
      "source": [
        "\n",
        "tc_algo_folder= \"inceptionV3\"\n",
        "#tc_img_dim=(224, 224)\n",
        "tc_img_dim=(299, 299)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRZPsC9iI5Gp"
      },
      "source": [
        " \n",
        "# Some setup for trainning \n",
        "1.   Defining the number of epochs and batch size\n",
        "2.   Early stoping and checkpoint to avoid overfitting \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B79V0ONSUNm"
      },
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 200\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SbT17ewJgqI"
      },
      "source": [
        "# **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbYZsbdHTxls"
      },
      "source": [
        "# Here we load data from pickels\n",
        "X,Y_= pg.get_data(str(tc_img_dim[0]))\n",
        "Y = to_categorical(Y_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua62WHYq5JZR",
        "outputId": "b5baab51-4196-4062-90f5-7dee47e448bc"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4000, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activate the TPU**"
      ],
      "metadata": {
        "id": "1A2vaMqufuts"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSCIPGPU0qEv"
      },
      "source": [
        "use_tpu = True\n",
        "import os\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TF_MASTER=''\n",
        "\n",
        "# TPU address\n",
        "tpu_address = TF_MASTER\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyPwOCTzdhTS",
        "outputId": "ab2c86fb-fc39-4f51-820f-c14d8d4cbdc5"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vrxiAbFdq0d",
        "outputId": "cc730d14-e914-4936-8dbb-aaf74e70d22d"
      },
      "source": [
        "%cd /content/drive/MyDrive/Breast_Cancer/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Breast_Cancer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpXLKv7WJ0YI"
      },
      "source": [
        "## **Training + KFOLD**\n",
        "\n",
        "\n",
        "1.  First we defind fold_y / fold_y_pred / histories where we store training history \n",
        "2.   We use 5fold stratified to avoid overfitting and make sure that the results are good\n",
        "3. We call the training_pg where we defined the training \n",
        "4. Since the training is super fast we can keep checkpoint od delet it (Up to you) Personnly I dont use it / In case you use GPU or CPU checkpoint is mondatory :) \n",
        "5. We make sure to delet the x_train y_train x_test y_test and model to liberate the RAM (since everything is saved) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcEWdPOnT2vV",
        "outputId": "890faccf-6ed9-4bf9-a1e3-d88c9d4ddf83"
      },
      "source": [
        "skf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
        "    \n",
        "folds_y = []\n",
        "folds_y_pred = []\n",
        "histories= []\n",
        "fold = 0\n",
        "for train, test in skf.split(X,Y_):\n",
        "    fold+=1\n",
        "    print(\"########################################################################\")\n",
        "    print(f\"Fold #{fold}\")\n",
        "    print(\"########################################################################\")\n",
        "    x_train = X[train]\n",
        "    y_train = Y[train]\n",
        "    x_test = X[test]\n",
        "    y_test = Y[test]\n",
        "        \n",
        "    with strategy.scope():\n",
        "      if tc_algo_folder == \"Baseline\":\n",
        "        model = pg.get_baseline()\n",
        "      else :\n",
        "        tc_base_cnv= pg.base_conv(tc_algo_folder)\n",
        "        model = pg.get_model(tc_base_cnv,trainable=False)\n",
        "      \n",
        "    checkpoint = ModelCheckpoint( filepath='results/vectors/'+tc_algo_folder+'/fold_'+str(fold)+'.h5', monitor='val_accuracy', save_best_only=True, verbose= 1, save_weights_only=False)\n",
        "    history= model.fit(x_train,y_train,validation_data=(x_test,y_test), epochs=epochs,batch_size = batch_size, callbacks=[reduce_lr,early_stopping, checkpoint])\n",
        "    del model\n",
        "    #loading best classifier and save it to my drive\n",
        "    model= load_model('/content/drive/MyDrive/Breast_Cancer/results/vectors/'+tc_algo_folder+'/fold_'+str(fold)+'.h5')\n",
        "    pred = model.predict(x_test)\n",
        "    folds_y.append(y_test)\n",
        "    folds_y_pred.append(pred)\n",
        "    histories.append(history.history)\n",
        "    del model, x_train, y_train, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########################################################################\n",
            "Fold #1\n",
            "########################################################################\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-mobile-no-top.h5\n",
            "19996672/19993432 [==============================] - 0s 0us/step\n",
            "Epoch 1/200\n",
            "100/100 [==============================] - 56s 259ms/step - loss: 10.6827 - accuracy: 0.6269 - val_loss: 10.2070 - val_accuracy: 0.7487\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.74875, saving model to results/vectors/nasNet/fold_1.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 10.0492 - accuracy: 0.7328 - val_loss: 9.8041 - val_accuracy: 0.7337\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.74875\n",
            "Epoch 3/200\n",
            "100/100 [==============================] - 6s 65ms/step - loss: 9.5886 - accuracy: 0.7653 - val_loss: 9.4023 - val_accuracy: 0.7763\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.74875 to 0.77625, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 4/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 9.1696 - accuracy: 0.7994 - val_loss: 9.0380 - val_accuracy: 0.7675\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.77625\n",
            "Epoch 5/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 8.7848 - accuracy: 0.8247 - val_loss: 8.6940 - val_accuracy: 0.7837\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.77625 to 0.78375, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 6/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 8.4288 - accuracy: 0.8462 - val_loss: 8.3715 - val_accuracy: 0.7937\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.78375 to 0.79375, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 7/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 8.0909 - accuracy: 0.8609 - val_loss: 8.0684 - val_accuracy: 0.7850\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.79375\n",
            "Epoch 8/200\n",
            "100/100 [==============================] - 6s 63ms/step - loss: 7.7645 - accuracy: 0.8794 - val_loss: 7.7806 - val_accuracy: 0.7912\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.79375\n",
            "Epoch 9/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 7.4803 - accuracy: 0.8825 - val_loss: 7.5084 - val_accuracy: 0.7875\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.79375\n",
            "Epoch 10/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 7.1987 - accuracy: 0.8969 - val_loss: 7.2567 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.79375 to 0.80000, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 11/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 6.9304 - accuracy: 0.9044 - val_loss: 7.0084 - val_accuracy: 0.7975\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.80000\n",
            "Epoch 12/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 6.6770 - accuracy: 0.9219 - val_loss: 6.7828 - val_accuracy: 0.7950\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.80000\n",
            "Epoch 13/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 6.4433 - accuracy: 0.9244 - val_loss: 6.5670 - val_accuracy: 0.7937\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.80000\n",
            "Epoch 14/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 6.2258 - accuracy: 0.9306 - val_loss: 6.3641 - val_accuracy: 0.7900\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.80000\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 6.0146 - accuracy: 0.9375 - val_loss: 6.1614 - val_accuracy: 0.7975\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.80000\n",
            "Epoch 16/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 5.8119 - accuracy: 0.9478 - val_loss: 5.9863 - val_accuracy: 0.7900\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.80000\n",
            "Epoch 17/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 5.6236 - accuracy: 0.9500 - val_loss: 5.8157 - val_accuracy: 0.7900\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.80000\n",
            "Epoch 18/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 5.4469 - accuracy: 0.9553 - val_loss: 5.6487 - val_accuracy: 0.7950\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.80000\n",
            "Epoch 19/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 5.2717 - accuracy: 0.9606 - val_loss: 5.4812 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.80000 to 0.80750, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 20/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 5.1146 - accuracy: 0.9575 - val_loss: 5.3370 - val_accuracy: 0.7975\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.80750\n",
            "Epoch 21/200\n",
            "100/100 [==============================] - 6s 65ms/step - loss: 4.9560 - accuracy: 0.9650 - val_loss: 5.1947 - val_accuracy: 0.7975\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.80750\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 4.8084 - accuracy: 0.9688 - val_loss: 5.0605 - val_accuracy: 0.7962\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.80750\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 4.6663 - accuracy: 0.9697 - val_loss: 4.9332 - val_accuracy: 0.8025\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.80750\n",
            "Epoch 24/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 4.5359 - accuracy: 0.9725 - val_loss: 4.7980 - val_accuracy: 0.8025\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.80750\n",
            "Epoch 25/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 4.4071 - accuracy: 0.9753 - val_loss: 4.6777 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.80750\n",
            "Epoch 26/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 4.2778 - accuracy: 0.9784 - val_loss: 4.5609 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.80750 to 0.80875, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 27/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 4.1580 - accuracy: 0.9828 - val_loss: 4.4516 - val_accuracy: 0.8025\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.80875\n",
            "Epoch 28/200\n",
            "100/100 [==============================] - 7s 74ms/step - loss: 4.0471 - accuracy: 0.9822 - val_loss: 4.3548 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.80875\n",
            "Epoch 29/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 3.9359 - accuracy: 0.9844 - val_loss: 4.2412 - val_accuracy: 0.8025\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.80875\n",
            "Epoch 30/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 3.8282 - accuracy: 0.9828 - val_loss: 4.1428 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.80875\n",
            "Epoch 31/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 3.7278 - accuracy: 0.9856 - val_loss: 4.0403 - val_accuracy: 0.8025\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.80875\n",
            "Epoch 32/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 3.6279 - accuracy: 0.9884 - val_loss: 3.9528 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.80875\n",
            "Epoch 33/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 3.5364 - accuracy: 0.9875 - val_loss: 3.8789 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.80875\n",
            "Epoch 34/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.4407 - accuracy: 0.9891 - val_loss: 3.7971 - val_accuracy: 0.8025\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.80875\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.3521 - accuracy: 0.9912 - val_loss: 3.6943 - val_accuracy: 0.8025\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.80875\n",
            "Epoch 36/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.2632 - accuracy: 0.9941 - val_loss: 3.6216 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.80875\n",
            "Epoch 37/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.1796 - accuracy: 0.9931 - val_loss: 3.5331 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.80875\n",
            "Epoch 38/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.0956 - accuracy: 0.9953 - val_loss: 3.4559 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.80875\n",
            "Epoch 39/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 3.0157 - accuracy: 0.9962 - val_loss: 3.3919 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00039: val_accuracy improved from 0.80875 to 0.81375, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 40/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 2.9438 - accuracy: 0.9934 - val_loss: 3.3001 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00040: val_accuracy improved from 0.81375 to 0.81500, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 41/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.8646 - accuracy: 0.9950 - val_loss: 3.2354 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.81500\n",
            "Epoch 42/200\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 2.7941 - accuracy: 0.9953 - val_loss: 3.1580 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.81500\n",
            "Epoch 43/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.7214 - accuracy: 0.9966 - val_loss: 3.0952 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.81500\n",
            "Epoch 44/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.6546 - accuracy: 0.9947 - val_loss: 3.0329 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.81500\n",
            "Epoch 45/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 2.5831 - accuracy: 0.9969 - val_loss: 2.9615 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.81500\n",
            "Epoch 46/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 2.5206 - accuracy: 0.9966 - val_loss: 2.9096 - val_accuracy: 0.7975\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.81500\n",
            "Epoch 47/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 2.4567 - accuracy: 0.9984 - val_loss: 2.8457 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.81500\n",
            "Epoch 48/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 2.3956 - accuracy: 0.9969 - val_loss: 2.7855 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.81500\n",
            "Epoch 49/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.3343 - accuracy: 0.9978 - val_loss: 2.7302 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.81500\n",
            "Epoch 50/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.2730 - accuracy: 0.9984 - val_loss: 2.6761 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.81500\n",
            "Epoch 51/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.2170 - accuracy: 0.9994 - val_loss: 2.6208 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.81500\n",
            "Epoch 52/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.1640 - accuracy: 0.9978 - val_loss: 2.5697 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.81500\n",
            "Epoch 53/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.1106 - accuracy: 0.9978 - val_loss: 2.5247 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.81500\n",
            "Epoch 54/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.0571 - accuracy: 0.9978 - val_loss: 2.4593 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.81500\n",
            "Epoch 55/200\n",
            "100/100 [==============================] - 6s 62ms/step - loss: 2.0070 - accuracy: 0.9987 - val_loss: 2.4158 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.81500\n",
            "Epoch 56/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.9562 - accuracy: 0.9981 - val_loss: 2.3706 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.81500\n",
            "Epoch 57/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.9081 - accuracy: 0.9981 - val_loss: 2.3158 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.81500\n",
            "Epoch 58/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.8612 - accuracy: 0.9975 - val_loss: 2.2830 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.81500\n",
            "Epoch 59/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 1.8152 - accuracy: 0.9987 - val_loss: 2.2317 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.81500\n",
            "Epoch 60/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.7702 - accuracy: 0.9984 - val_loss: 2.1972 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.81500\n",
            "Epoch 61/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.7282 - accuracy: 0.9984 - val_loss: 2.1519 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.81500\n",
            "Epoch 62/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.6861 - accuracy: 0.9984 - val_loss: 2.1106 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.81500\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.6448 - accuracy: 0.9984 - val_loss: 2.0753 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.81500\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 1.6045 - accuracy: 0.9991 - val_loss: 2.0403 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.81500\n",
            "Epoch 65/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 1.5645 - accuracy: 0.9997 - val_loss: 2.0047 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.81500\n",
            "Epoch 66/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.5281 - accuracy: 0.9997 - val_loss: 1.9686 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.81500\n",
            "Epoch 67/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.4934 - accuracy: 0.9991 - val_loss: 1.9292 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00067: val_accuracy improved from 0.81500 to 0.81625, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 68/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.4577 - accuracy: 0.9994 - val_loss: 1.9001 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.81625\n",
            "Epoch 69/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.4232 - accuracy: 0.9991 - val_loss: 1.8616 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.81625\n",
            "Epoch 70/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.3900 - accuracy: 1.0000 - val_loss: 1.8368 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.81625\n",
            "Epoch 71/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.3575 - accuracy: 0.9991 - val_loss: 1.7999 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.81625\n",
            "Epoch 72/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.3251 - accuracy: 0.9997 - val_loss: 1.7758 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00072: val_accuracy improved from 0.81625 to 0.81875, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 73/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.2976 - accuracy: 0.9981 - val_loss: 1.7427 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.81875\n",
            "Epoch 74/200\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 1.2648 - accuracy: 0.9997 - val_loss: 1.7118 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.81875\n",
            "Epoch 75/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 1.2361 - accuracy: 0.9994 - val_loss: 1.6882 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.81875\n",
            "Epoch 76/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 1.2097 - accuracy: 0.9994 - val_loss: 1.6649 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.81875\n",
            "Epoch 77/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 1.1844 - accuracy: 0.9994 - val_loss: 1.6415 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.81875\n",
            "Epoch 78/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.1559 - accuracy: 0.9997 - val_loss: 1.6067 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.81875\n",
            "Epoch 79/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 1.1297 - accuracy: 1.0000 - val_loss: 1.5838 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.81875\n",
            "Epoch 80/200\n",
            "100/100 [==============================] - 6s 62ms/step - loss: 1.1031 - accuracy: 0.9994 - val_loss: 1.5669 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.81875\n",
            "Epoch 81/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0815 - accuracy: 0.9997 - val_loss: 1.5459 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.81875\n",
            "Epoch 82/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.0597 - accuracy: 0.9984 - val_loss: 1.5094 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.81875\n",
            "Epoch 83/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.0359 - accuracy: 0.9997 - val_loss: 1.5128 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.81875\n",
            "Epoch 84/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 1.0134 - accuracy: 0.9997 - val_loss: 1.4843 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.81875\n",
            "Epoch 85/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.9933 - accuracy: 1.0000 - val_loss: 1.4595 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.81875\n",
            "Epoch 86/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.9716 - accuracy: 0.9997 - val_loss: 1.4399 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.81875\n",
            "Epoch 87/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.9517 - accuracy: 0.9997 - val_loss: 1.4194 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.81875\n",
            "Epoch 88/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.9323 - accuracy: 0.9997 - val_loss: 1.3940 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.81875\n",
            "Epoch 89/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.9140 - accuracy: 0.9997 - val_loss: 1.3706 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.81875\n",
            "Epoch 90/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.8942 - accuracy: 1.0000 - val_loss: 1.3627 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.81875\n",
            "Epoch 91/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8768 - accuracy: 1.0000 - val_loss: 1.3495 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.81875\n",
            "Epoch 92/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8595 - accuracy: 0.9997 - val_loss: 1.3257 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.81875\n",
            "Epoch 93/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.8433 - accuracy: 0.9997 - val_loss: 1.3142 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.81875\n",
            "Epoch 94/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8247 - accuracy: 1.0000 - val_loss: 1.2907 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.81875\n",
            "Epoch 95/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8093 - accuracy: 1.0000 - val_loss: 1.2939 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.81875\n",
            "Epoch 96/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.7940 - accuracy: 1.0000 - val_loss: 1.2628 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.81875\n",
            "Epoch 97/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.7779 - accuracy: 1.0000 - val_loss: 1.2603 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.81875\n",
            "Epoch 98/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.7639 - accuracy: 1.0000 - val_loss: 1.2411 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.81875\n",
            "Epoch 99/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.7503 - accuracy: 0.9991 - val_loss: 1.2183 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.81875\n",
            "Epoch 100/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.7349 - accuracy: 0.9997 - val_loss: 1.2084 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00100: val_accuracy improved from 0.81875 to 0.82125, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 101/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.7220 - accuracy: 1.0000 - val_loss: 1.2161 - val_accuracy: 0.8025\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.82125\n",
            "Epoch 102/200\n",
            "100/100 [==============================] - 7s 71ms/step - loss: 0.7084 - accuracy: 1.0000 - val_loss: 1.1810 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.82125\n",
            "Epoch 103/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.6949 - accuracy: 1.0000 - val_loss: 1.1636 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.82125\n",
            "Epoch 104/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.6816 - accuracy: 1.0000 - val_loss: 1.1596 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.82125\n",
            "Epoch 105/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.6698 - accuracy: 0.9997 - val_loss: 1.1595 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.82125\n",
            "Epoch 106/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.6580 - accuracy: 1.0000 - val_loss: 1.1419 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.82125\n",
            "Epoch 107/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 0.6472 - accuracy: 0.9991 - val_loss: 1.1429 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.82125\n",
            "Epoch 108/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.6349 - accuracy: 0.9997 - val_loss: 1.1230 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.82125\n",
            "Epoch 109/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.6238 - accuracy: 1.0000 - val_loss: 1.1167 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.82125\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.6131 - accuracy: 1.0000 - val_loss: 1.0982 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.82125\n",
            "Epoch 111/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.6016 - accuracy: 1.0000 - val_loss: 1.0907 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.82125\n",
            "Epoch 112/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.5913 - accuracy: 1.0000 - val_loss: 1.0748 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.82125\n",
            "Epoch 113/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5810 - accuracy: 1.0000 - val_loss: 1.0781 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.82125\n",
            "Epoch 114/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.5716 - accuracy: 1.0000 - val_loss: 1.0681 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.82125\n",
            "Epoch 115/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5631 - accuracy: 0.9997 - val_loss: 1.0410 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.82125\n",
            "Epoch 116/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5548 - accuracy: 1.0000 - val_loss: 1.0359 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.82125\n",
            "Epoch 117/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.5457 - accuracy: 0.9997 - val_loss: 1.0308 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.82125\n",
            "Epoch 118/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.5358 - accuracy: 1.0000 - val_loss: 1.0321 - val_accuracy: 0.8037\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.82125\n",
            "Epoch 119/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.5285 - accuracy: 0.9991 - val_loss: 1.0125 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.82125\n",
            "Epoch 120/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.5189 - accuracy: 1.0000 - val_loss: 1.0084 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.82125\n",
            "Epoch 121/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.5110 - accuracy: 1.0000 - val_loss: 0.9840 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00121: val_accuracy improved from 0.82125 to 0.82375, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 122/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.5031 - accuracy: 1.0000 - val_loss: 0.9792 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.82375\n",
            "Epoch 123/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 0.4953 - accuracy: 1.0000 - val_loss: 0.9891 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.82375\n",
            "Epoch 124/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4871 - accuracy: 1.0000 - val_loss: 0.9884 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.82375\n",
            "Epoch 125/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4816 - accuracy: 1.0000 - val_loss: 0.9844 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.82375\n",
            "Epoch 126/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4743 - accuracy: 0.9997 - val_loss: 0.9937 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.82375\n",
            "Epoch 127/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4677 - accuracy: 0.9997 - val_loss: 0.9631 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.82375\n",
            "Epoch 128/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4608 - accuracy: 0.9997 - val_loss: 0.9587 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.82375\n",
            "Epoch 129/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4554 - accuracy: 1.0000 - val_loss: 0.9475 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.82375\n",
            "Epoch 130/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4474 - accuracy: 1.0000 - val_loss: 0.9338 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.82375\n",
            "Epoch 131/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.4403 - accuracy: 1.0000 - val_loss: 0.9416 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.82375\n",
            "Epoch 132/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.4336 - accuracy: 1.0000 - val_loss: 0.9225 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.82375\n",
            "Epoch 133/200\n",
            "100/100 [==============================] - 6s 61ms/step - loss: 0.4277 - accuracy: 1.0000 - val_loss: 0.9381 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.82375\n",
            "Epoch 134/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.4224 - accuracy: 1.0000 - val_loss: 0.9256 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.82375\n",
            "Epoch 135/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.4169 - accuracy: 1.0000 - val_loss: 0.9245 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.82375\n",
            "Epoch 136/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.4103 - accuracy: 1.0000 - val_loss: 0.8977 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.82375\n",
            "Epoch 137/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.4055 - accuracy: 1.0000 - val_loss: 0.9141 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.82375\n",
            "Epoch 138/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.3990 - accuracy: 1.0000 - val_loss: 0.8994 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.82375\n",
            "Epoch 139/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.3927 - accuracy: 1.0000 - val_loss: 0.9008 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.82375\n",
            "Epoch 140/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.3892 - accuracy: 1.0000 - val_loss: 0.9057 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.82375\n",
            "Epoch 141/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3865 - accuracy: 1.0000 - val_loss: 0.8798 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.82375\n",
            "Epoch 142/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3815 - accuracy: 1.0000 - val_loss: 0.8703 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.82375\n",
            "Epoch 143/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3732 - accuracy: 1.0000 - val_loss: 0.8732 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.82375\n",
            "Epoch 144/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3686 - accuracy: 1.0000 - val_loss: 0.8725 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.82375\n",
            "Epoch 145/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3646 - accuracy: 1.0000 - val_loss: 0.8508 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.82375\n",
            "Epoch 146/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3588 - accuracy: 1.0000 - val_loss: 0.8625 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.82375\n",
            "Epoch 147/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3562 - accuracy: 0.9997 - val_loss: 0.8579 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.82375\n",
            "Epoch 148/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3518 - accuracy: 1.0000 - val_loss: 0.8632 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.82375\n",
            "Epoch 149/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3465 - accuracy: 1.0000 - val_loss: 0.8370 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.82375\n",
            "Epoch 150/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3426 - accuracy: 1.0000 - val_loss: 0.8431 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.82375\n",
            "Epoch 151/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3400 - accuracy: 1.0000 - val_loss: 0.8417 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.82375\n",
            "Epoch 152/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3338 - accuracy: 1.0000 - val_loss: 0.8281 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.82375\n",
            "Epoch 153/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3306 - accuracy: 1.0000 - val_loss: 0.8435 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.82375\n",
            "Epoch 154/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3265 - accuracy: 1.0000 - val_loss: 0.8189 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.82375\n",
            "Epoch 155/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.3224 - accuracy: 1.0000 - val_loss: 0.8261 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.82375\n",
            "Epoch 156/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.3208 - accuracy: 0.9997 - val_loss: 0.8145 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.82375\n",
            "Epoch 157/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.8097 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.82375\n",
            "Epoch 158/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.3121 - accuracy: 1.0000 - val_loss: 0.8216 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.82375\n",
            "Epoch 159/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.3121 - accuracy: 0.9994 - val_loss: 0.8178 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.82375\n",
            "Epoch 160/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.3056 - accuracy: 1.0000 - val_loss: 0.8122 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.82375\n",
            "Epoch 161/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.3019 - accuracy: 1.0000 - val_loss: 0.8054 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.82375\n",
            "Epoch 162/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 0.2982 - accuracy: 1.0000 - val_loss: 0.8028 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.82375\n",
            "Epoch 163/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2944 - accuracy: 1.0000 - val_loss: 0.8090 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.82375\n",
            "Epoch 164/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2912 - accuracy: 1.0000 - val_loss: 0.7905 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.82375\n",
            "Epoch 165/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2884 - accuracy: 1.0000 - val_loss: 0.7891 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.82375\n",
            "Epoch 166/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2860 - accuracy: 1.0000 - val_loss: 0.7932 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.82375\n",
            "Epoch 167/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2826 - accuracy: 0.9997 - val_loss: 0.7922 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.82375\n",
            "Epoch 168/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2801 - accuracy: 1.0000 - val_loss: 0.7867 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.82375\n",
            "Epoch 169/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2778 - accuracy: 1.0000 - val_loss: 0.7948 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.82375\n",
            "Epoch 170/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2746 - accuracy: 0.9997 - val_loss: 0.7932 - val_accuracy: 0.8037\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.82375\n",
            "Epoch 171/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.2717 - accuracy: 1.0000 - val_loss: 0.7717 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.82375\n",
            "Epoch 172/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2680 - accuracy: 0.9994 - val_loss: 0.7719 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.82375\n",
            "Epoch 173/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2663 - accuracy: 1.0000 - val_loss: 0.7745 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.82375\n",
            "Epoch 174/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2624 - accuracy: 1.0000 - val_loss: 0.7753 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.82375\n",
            "Epoch 175/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2611 - accuracy: 1.0000 - val_loss: 0.7753 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.82375\n",
            "Epoch 176/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2597 - accuracy: 0.9994 - val_loss: 0.7767 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.82375\n",
            "Epoch 177/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.2568 - accuracy: 1.0000 - val_loss: 0.7610 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.82375\n",
            "Epoch 178/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.2531 - accuracy: 1.0000 - val_loss: 0.7624 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.82375\n",
            "Epoch 179/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2509 - accuracy: 1.0000 - val_loss: 0.7705 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.82375\n",
            "Epoch 180/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2478 - accuracy: 1.0000 - val_loss: 0.7449 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00180: val_accuracy improved from 0.82375 to 0.82500, saving model to results/vectors/nasNet/fold_1.h5\n",
            "Epoch 181/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2471 - accuracy: 1.0000 - val_loss: 0.7736 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.82500\n",
            "Epoch 182/200\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 0.2426 - accuracy: 1.0000 - val_loss: 0.7808 - val_accuracy: 0.8037\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.82500\n",
            "Epoch 183/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2420 - accuracy: 0.9997 - val_loss: 0.7559 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.82500\n",
            "Epoch 184/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2390 - accuracy: 1.0000 - val_loss: 0.7526 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.82500\n",
            "Epoch 185/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2369 - accuracy: 1.0000 - val_loss: 0.7492 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.82500\n",
            "Epoch 186/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2340 - accuracy: 1.0000 - val_loss: 0.7486 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.82500\n",
            "Epoch 187/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2317 - accuracy: 1.0000 - val_loss: 0.7523 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.82500\n",
            "Epoch 188/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2298 - accuracy: 1.0000 - val_loss: 0.7580 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.82500\n",
            "Epoch 189/200\n",
            "100/100 [==============================] - 6s 62ms/step - loss: 0.2296 - accuracy: 1.0000 - val_loss: 0.7493 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.82500\n",
            "Epoch 190/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2268 - accuracy: 1.0000 - val_loss: 0.7516 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00190: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-07.\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.82500\n",
            "Epoch 191/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2240 - accuracy: 1.0000 - val_loss: 0.7441 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.82500\n",
            "Epoch 192/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2216 - accuracy: 1.0000 - val_loss: 0.7436 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.82500\n",
            "Epoch 193/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2214 - accuracy: 1.0000 - val_loss: 0.7413 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.82500\n",
            "Epoch 194/200\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.2208 - accuracy: 1.0000 - val_loss: 0.7447 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.82500\n",
            "Epoch 195/200\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.2208 - accuracy: 1.0000 - val_loss: 0.7417 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.82500\n",
            "Epoch 196/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2201 - accuracy: 1.0000 - val_loss: 0.7408 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.82500\n",
            "Epoch 197/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2193 - accuracy: 1.0000 - val_loss: 0.7427 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.82500\n",
            "Epoch 198/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2198 - accuracy: 1.0000 - val_loss: 0.7423 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.82500\n",
            "Epoch 199/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2194 - accuracy: 1.0000 - val_loss: 0.7415 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.82500\n",
            "Epoch 200/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.2186 - accuracy: 1.0000 - val_loss: 0.7420 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.82500\n",
            "########################################################################\n",
            "Fold #2\n",
            "########################################################################\n",
            "Epoch 1/200\n",
            "100/100 [==============================] - 58s 269ms/step - loss: 10.7217 - accuracy: 0.6141 - val_loss: 10.1980 - val_accuracy: 0.7387\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.73875, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 2/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 10.0196 - accuracy: 0.7325 - val_loss: 9.7517 - val_accuracy: 0.7763\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.73875 to 0.77625, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 3/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 9.5619 - accuracy: 0.7628 - val_loss: 9.3689 - val_accuracy: 0.7625\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.77625\n",
            "Epoch 4/200\n",
            "100/100 [==============================] - 7s 69ms/step - loss: 9.1367 - accuracy: 0.7912 - val_loss: 8.9718 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.77625 to 0.80625, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 5/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 8.7439 - accuracy: 0.8075 - val_loss: 8.6139 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.80625 to 0.80875, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 6/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 8.3748 - accuracy: 0.8388 - val_loss: 8.2856 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.80875\n",
            "Epoch 7/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 8.0365 - accuracy: 0.8497 - val_loss: 7.9750 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.80875 to 0.81250, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 8/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 7.7135 - accuracy: 0.8644 - val_loss: 7.6811 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.81250\n",
            "Epoch 9/200\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 7.4101 - accuracy: 0.8778 - val_loss: 7.4371 - val_accuracy: 0.7875\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.81250\n",
            "Epoch 10/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 7.1234 - accuracy: 0.8925 - val_loss: 7.1446 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.81250 to 0.81375, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 11/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 6.8494 - accuracy: 0.9094 - val_loss: 6.9003 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.81375\n",
            "Epoch 12/200\n",
            "100/100 [==============================] - 6s 63ms/step - loss: 6.6040 - accuracy: 0.9112 - val_loss: 6.6713 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.81375 to 0.81625, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 13/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 6.3617 - accuracy: 0.9159 - val_loss: 6.4419 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.81625 to 0.82250, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 14/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 6.1378 - accuracy: 0.9294 - val_loss: 6.2529 - val_accuracy: 0.8025\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.82250\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 5.9331 - accuracy: 0.9319 - val_loss: 6.0478 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.82250\n",
            "Epoch 16/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 5.7279 - accuracy: 0.9350 - val_loss: 5.8541 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.82250 to 0.82750, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 17/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 5.5365 - accuracy: 0.9478 - val_loss: 5.6812 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.82750\n",
            "Epoch 18/200\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 5.3614 - accuracy: 0.9466 - val_loss: 5.5172 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.82750\n",
            "Epoch 19/200\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 5.1843 - accuracy: 0.9578 - val_loss: 5.3625 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.82750\n",
            "Epoch 20/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 5.0198 - accuracy: 0.9616 - val_loss: 5.2154 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.82750\n",
            "Epoch 21/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 4.8717 - accuracy: 0.9597 - val_loss: 5.0634 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.82750\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 4.7208 - accuracy: 0.9716 - val_loss: 4.9436 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.82750\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 4.5841 - accuracy: 0.9716 - val_loss: 4.8097 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.82750\n",
            "Epoch 24/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 4.4461 - accuracy: 0.9750 - val_loss: 4.6714 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.82750 to 0.82875, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 25/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 4.3169 - accuracy: 0.9747 - val_loss: 4.5490 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.82875\n",
            "Epoch 26/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 4.1987 - accuracy: 0.9791 - val_loss: 4.4460 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.82875\n",
            "Epoch 27/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 4.0808 - accuracy: 0.9812 - val_loss: 4.3288 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.82875\n",
            "Epoch 28/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.9653 - accuracy: 0.9837 - val_loss: 4.2302 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.82875\n",
            "Epoch 29/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.8572 - accuracy: 0.9837 - val_loss: 4.1263 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.82875\n",
            "Epoch 30/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.7511 - accuracy: 0.9881 - val_loss: 4.0233 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.82875\n",
            "Epoch 31/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.6486 - accuracy: 0.9884 - val_loss: 3.9615 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.82875\n",
            "Epoch 32/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.5576 - accuracy: 0.9859 - val_loss: 3.8380 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.82875\n",
            "Epoch 33/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.4607 - accuracy: 0.9903 - val_loss: 3.7464 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00033: val_accuracy improved from 0.82875 to 0.83250, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 34/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.3663 - accuracy: 0.9925 - val_loss: 3.6648 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.83250\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 3.2772 - accuracy: 0.9919 - val_loss: 3.5853 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.83250 to 0.83375, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 36/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.1947 - accuracy: 0.9919 - val_loss: 3.4993 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.83375\n",
            "Epoch 37/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 3.1107 - accuracy: 0.9934 - val_loss: 3.4248 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.83375 to 0.83500, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 38/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.0328 - accuracy: 0.9916 - val_loss: 3.3499 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.83500\n",
            "Epoch 39/200\n",
            "100/100 [==============================] - 6s 64ms/step - loss: 2.9537 - accuracy: 0.9947 - val_loss: 3.2788 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.83500\n",
            "Epoch 40/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.8734 - accuracy: 0.9959 - val_loss: 3.2118 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.83500\n",
            "Epoch 41/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.8034 - accuracy: 0.9941 - val_loss: 3.1422 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.83500\n",
            "Epoch 42/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.7321 - accuracy: 0.9953 - val_loss: 3.0646 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.83500\n",
            "Epoch 43/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.6634 - accuracy: 0.9962 - val_loss: 3.0005 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.83500\n",
            "Epoch 44/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.5982 - accuracy: 0.9966 - val_loss: 2.9407 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.83500\n",
            "Epoch 45/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.5305 - accuracy: 0.9947 - val_loss: 2.8777 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.83500\n",
            "Epoch 46/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.4666 - accuracy: 0.9975 - val_loss: 2.8290 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.83500\n",
            "Epoch 47/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.4039 - accuracy: 0.9969 - val_loss: 2.7708 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.83500\n",
            "Epoch 48/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.3463 - accuracy: 0.9959 - val_loss: 2.7060 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.83500\n",
            "Epoch 49/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.2837 - accuracy: 0.9981 - val_loss: 2.6518 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.83500\n",
            "Epoch 50/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.2302 - accuracy: 0.9966 - val_loss: 2.5972 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.83500\n",
            "Epoch 51/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.1735 - accuracy: 0.9978 - val_loss: 2.5384 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.83500\n",
            "Epoch 52/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.1185 - accuracy: 0.9981 - val_loss: 2.4905 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.83500\n",
            "Epoch 53/200\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 2.0678 - accuracy: 0.9984 - val_loss: 2.4320 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.83500\n",
            "Epoch 54/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.0177 - accuracy: 0.9981 - val_loss: 2.3941 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.83500\n",
            "Epoch 55/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.9676 - accuracy: 0.9975 - val_loss: 2.3444 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.83500\n",
            "Epoch 56/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.9195 - accuracy: 0.9981 - val_loss: 2.3042 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.83500\n",
            "Epoch 57/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.8740 - accuracy: 0.9978 - val_loss: 2.2681 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.83500\n",
            "Epoch 58/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.8280 - accuracy: 0.9978 - val_loss: 2.2052 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.83500\n",
            "Epoch 59/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.7823 - accuracy: 0.9991 - val_loss: 2.1767 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.83500\n",
            "Epoch 60/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.7422 - accuracy: 0.9981 - val_loss: 2.1243 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.83500\n",
            "Epoch 61/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.6981 - accuracy: 0.9997 - val_loss: 2.0862 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.83500\n",
            "Epoch 62/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.6573 - accuracy: 0.9991 - val_loss: 2.0561 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.83500\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.6190 - accuracy: 0.9994 - val_loss: 2.0115 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.83500\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.5792 - accuracy: 0.9994 - val_loss: 1.9790 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.83500\n",
            "Epoch 65/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.5440 - accuracy: 0.9987 - val_loss: 1.9430 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.83500\n",
            "Epoch 66/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.5067 - accuracy: 0.9987 - val_loss: 1.9070 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.83500\n",
            "Epoch 67/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.4742 - accuracy: 0.9981 - val_loss: 1.8819 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.83500\n",
            "Epoch 68/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.4371 - accuracy: 0.9997 - val_loss: 1.8522 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.83500\n",
            "Epoch 69/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.4016 - accuracy: 0.9997 - val_loss: 1.8119 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.83500\n",
            "Epoch 70/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.3730 - accuracy: 1.0000 - val_loss: 1.7798 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.83500\n",
            "Epoch 71/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.3413 - accuracy: 0.9991 - val_loss: 1.7482 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00071: val_accuracy improved from 0.83500 to 0.83625, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 72/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.3105 - accuracy: 1.0000 - val_loss: 1.7251 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.83625\n",
            "Epoch 73/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 1.2796 - accuracy: 0.9994 - val_loss: 1.6966 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.83625\n",
            "Epoch 74/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.2514 - accuracy: 0.9994 - val_loss: 1.6711 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.83625\n",
            "Epoch 75/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.2257 - accuracy: 0.9994 - val_loss: 1.6569 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.83625\n",
            "Epoch 76/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.1966 - accuracy: 0.9991 - val_loss: 1.6295 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.83625\n",
            "Epoch 77/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.1714 - accuracy: 0.9997 - val_loss: 1.5992 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.83625\n",
            "Epoch 78/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.1450 - accuracy: 1.0000 - val_loss: 1.5643 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.83625\n",
            "Epoch 79/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.1198 - accuracy: 0.9994 - val_loss: 1.5418 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.83625\n",
            "Epoch 80/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.0966 - accuracy: 0.9994 - val_loss: 1.5120 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.83625\n",
            "Epoch 81/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0736 - accuracy: 0.9994 - val_loss: 1.5009 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.83625\n",
            "Epoch 82/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0492 - accuracy: 0.9994 - val_loss: 1.4725 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.83625\n",
            "Epoch 83/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.0264 - accuracy: 0.9997 - val_loss: 1.4538 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.83625\n",
            "Epoch 84/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0083 - accuracy: 0.9991 - val_loss: 1.4413 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.83625\n",
            "Epoch 85/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.9850 - accuracy: 1.0000 - val_loss: 1.4305 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.83625\n",
            "Epoch 86/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.9651 - accuracy: 0.9991 - val_loss: 1.4207 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.83625\n",
            "Epoch 87/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.9455 - accuracy: 0.9997 - val_loss: 1.3776 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.83625\n",
            "Epoch 88/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.9249 - accuracy: 1.0000 - val_loss: 1.3623 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.83625\n",
            "Epoch 89/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.9062 - accuracy: 0.9991 - val_loss: 1.3416 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.83625\n",
            "Epoch 90/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8881 - accuracy: 0.9997 - val_loss: 1.3285 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.83625\n",
            "Epoch 91/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8722 - accuracy: 0.9991 - val_loss: 1.3048 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.83625\n",
            "Epoch 92/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8534 - accuracy: 1.0000 - val_loss: 1.3151 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.83625\n",
            "Epoch 93/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.8388 - accuracy: 1.0000 - val_loss: 1.2885 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.83625\n",
            "Epoch 94/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8228 - accuracy: 0.9991 - val_loss: 1.2775 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.83625\n",
            "Epoch 95/200\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.8054 - accuracy: 1.0000 - val_loss: 1.2527 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.83625\n",
            "Epoch 96/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7889 - accuracy: 1.0000 - val_loss: 1.2362 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.83625\n",
            "Epoch 97/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7752 - accuracy: 0.9997 - val_loss: 1.2127 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.83625\n",
            "Epoch 98/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.7605 - accuracy: 1.0000 - val_loss: 1.2132 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.83625\n",
            "Epoch 99/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7469 - accuracy: 0.9997 - val_loss: 1.1831 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.83625\n",
            "Epoch 100/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7329 - accuracy: 1.0000 - val_loss: 1.1880 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.83625\n",
            "Epoch 101/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7196 - accuracy: 1.0000 - val_loss: 1.1628 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.83625\n",
            "Epoch 102/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7054 - accuracy: 0.9997 - val_loss: 1.1506 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.83625\n",
            "Epoch 103/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.6932 - accuracy: 0.9994 - val_loss: 1.1407 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.83625\n",
            "Epoch 104/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.6805 - accuracy: 1.0000 - val_loss: 1.1236 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.83625\n",
            "Epoch 105/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.6680 - accuracy: 1.0000 - val_loss: 1.1044 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.83625\n",
            "Epoch 106/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.6565 - accuracy: 0.9997 - val_loss: 1.1025 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.83625\n",
            "Epoch 107/200\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.6464 - accuracy: 0.9994 - val_loss: 1.0993 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.83625\n",
            "Epoch 108/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6361 - accuracy: 0.9994 - val_loss: 1.0833 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.83625\n",
            "Epoch 109/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6230 - accuracy: 0.9994 - val_loss: 1.0696 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.83625\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6130 - accuracy: 0.9997 - val_loss: 1.0732 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.83625\n",
            "Epoch 111/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6035 - accuracy: 0.9997 - val_loss: 1.0595 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.83625\n",
            "Epoch 112/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5937 - accuracy: 0.9994 - val_loss: 1.0480 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.83625\n",
            "Epoch 113/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5836 - accuracy: 0.9991 - val_loss: 1.0443 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.83625\n",
            "Epoch 114/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.5734 - accuracy: 1.0000 - val_loss: 1.0248 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.83625\n",
            "Epoch 115/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5633 - accuracy: 1.0000 - val_loss: 1.0158 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.83625\n",
            "Epoch 116/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5544 - accuracy: 0.9997 - val_loss: 1.0036 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.83625\n",
            "Epoch 117/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5467 - accuracy: 0.9997 - val_loss: 0.9910 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.83625\n",
            "Epoch 118/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5383 - accuracy: 0.9994 - val_loss: 0.9922 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.83625\n",
            "Epoch 119/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5281 - accuracy: 1.0000 - val_loss: 0.9820 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.83625\n",
            "Epoch 120/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5198 - accuracy: 0.9997 - val_loss: 0.9838 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.83625\n",
            "Epoch 121/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5127 - accuracy: 1.0000 - val_loss: 0.9585 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.83625\n",
            "Epoch 122/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5051 - accuracy: 1.0000 - val_loss: 0.9523 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.83625\n",
            "Epoch 123/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4965 - accuracy: 1.0000 - val_loss: 0.9402 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.83625\n",
            "Epoch 124/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4889 - accuracy: 1.0000 - val_loss: 0.9522 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.83625\n",
            "Epoch 125/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4820 - accuracy: 1.0000 - val_loss: 0.9404 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.83625\n",
            "Epoch 126/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4751 - accuracy: 0.9994 - val_loss: 0.9219 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.83625\n",
            "Epoch 127/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4706 - accuracy: 0.9997 - val_loss: 0.9120 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.83625\n",
            "Epoch 128/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4617 - accuracy: 1.0000 - val_loss: 0.9435 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.83625\n",
            "Epoch 129/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4551 - accuracy: 0.9994 - val_loss: 0.9167 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.83625\n",
            "Epoch 130/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4475 - accuracy: 0.9997 - val_loss: 0.9061 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.83625\n",
            "Epoch 131/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4418 - accuracy: 0.9994 - val_loss: 0.8938 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.83625\n",
            "Epoch 132/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4338 - accuracy: 1.0000 - val_loss: 0.9204 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.83625\n",
            "Epoch 133/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4311 - accuracy: 0.9997 - val_loss: 0.8960 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.83625\n",
            "Epoch 134/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4227 - accuracy: 0.9997 - val_loss: 0.8878 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.83625\n",
            "Epoch 135/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4181 - accuracy: 0.9997 - val_loss: 0.8910 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.83625\n",
            "Epoch 136/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4118 - accuracy: 0.9997 - val_loss: 0.8702 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.83625\n",
            "Epoch 137/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4063 - accuracy: 1.0000 - val_loss: 0.8703 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.83625\n",
            "Epoch 138/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4017 - accuracy: 0.9997 - val_loss: 0.8637 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00138: val_accuracy improved from 0.83625 to 0.83750, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 139/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3961 - accuracy: 1.0000 - val_loss: 0.8745 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.83750\n",
            "Epoch 140/200\n",
            "100/100 [==============================] - 6s 61ms/step - loss: 0.3908 - accuracy: 0.9997 - val_loss: 0.8545 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.83750\n",
            "Epoch 141/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3845 - accuracy: 1.0000 - val_loss: 0.8441 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.83750\n",
            "Epoch 142/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3809 - accuracy: 1.0000 - val_loss: 0.8487 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.83750\n",
            "Epoch 143/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3762 - accuracy: 1.0000 - val_loss: 0.8415 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.83750\n",
            "Epoch 144/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3696 - accuracy: 1.0000 - val_loss: 0.8375 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.83750\n",
            "Epoch 145/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3652 - accuracy: 1.0000 - val_loss: 0.8300 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.83750\n",
            "Epoch 146/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3605 - accuracy: 1.0000 - val_loss: 0.8435 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.83750\n",
            "Epoch 147/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3574 - accuracy: 1.0000 - val_loss: 0.8405 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.83750\n",
            "Epoch 148/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3520 - accuracy: 1.0000 - val_loss: 0.8292 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.83750\n",
            "Epoch 149/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3484 - accuracy: 1.0000 - val_loss: 0.8543 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.83750\n",
            "Epoch 150/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3457 - accuracy: 1.0000 - val_loss: 0.8229 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.83750\n",
            "Epoch 151/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3396 - accuracy: 1.0000 - val_loss: 0.8190 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.83750\n",
            "Epoch 152/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3348 - accuracy: 1.0000 - val_loss: 0.8086 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.83750\n",
            "Epoch 153/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3340 - accuracy: 0.9997 - val_loss: 0.8061 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.83750\n",
            "Epoch 154/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3277 - accuracy: 1.0000 - val_loss: 0.7983 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.83750\n",
            "Epoch 155/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3248 - accuracy: 0.9997 - val_loss: 0.7945 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.83750\n",
            "Epoch 156/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3203 - accuracy: 1.0000 - val_loss: 0.7901 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.83750\n",
            "Epoch 157/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3169 - accuracy: 1.0000 - val_loss: 0.7923 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.83750\n",
            "Epoch 158/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3127 - accuracy: 1.0000 - val_loss: 0.8122 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.83750\n",
            "Epoch 159/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3087 - accuracy: 1.0000 - val_loss: 0.7780 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.83750\n",
            "Epoch 160/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3050 - accuracy: 1.0000 - val_loss: 0.7840 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.83750\n",
            "Epoch 161/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3016 - accuracy: 1.0000 - val_loss: 0.7796 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.83750\n",
            "Epoch 162/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3004 - accuracy: 1.0000 - val_loss: 0.7802 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.83750\n",
            "Epoch 163/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2967 - accuracy: 0.9997 - val_loss: 0.7730 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.83750\n",
            "Epoch 164/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2948 - accuracy: 0.9997 - val_loss: 0.7712 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.83750\n",
            "Epoch 165/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2898 - accuracy: 0.9997 - val_loss: 0.7657 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.83750\n",
            "Epoch 166/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2859 - accuracy: 1.0000 - val_loss: 0.7769 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.83750\n",
            "Epoch 167/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2826 - accuracy: 1.0000 - val_loss: 0.7670 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.83750\n",
            "Epoch 168/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2810 - accuracy: 1.0000 - val_loss: 0.7738 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.83750\n",
            "Epoch 169/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2770 - accuracy: 1.0000 - val_loss: 0.7605 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.83750\n",
            "Epoch 170/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2753 - accuracy: 1.0000 - val_loss: 0.7482 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.83750\n",
            "Epoch 171/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2733 - accuracy: 0.9997 - val_loss: 0.7470 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.83750\n",
            "Epoch 172/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2687 - accuracy: 1.0000 - val_loss: 0.7504 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.83750\n",
            "Epoch 173/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2672 - accuracy: 1.0000 - val_loss: 0.7484 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.83750\n",
            "Epoch 174/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2637 - accuracy: 1.0000 - val_loss: 0.7487 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.83750\n",
            "Epoch 175/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2610 - accuracy: 1.0000 - val_loss: 0.7336 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.83750\n",
            "Epoch 176/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2571 - accuracy: 1.0000 - val_loss: 0.7364 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.83750\n",
            "Epoch 177/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2592 - accuracy: 0.9994 - val_loss: 0.7333 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.83750\n",
            "Epoch 178/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2531 - accuracy: 1.0000 - val_loss: 0.7310 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.83750\n",
            "Epoch 179/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2509 - accuracy: 1.0000 - val_loss: 0.7546 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.83750\n",
            "Epoch 180/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2490 - accuracy: 1.0000 - val_loss: 0.7319 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00180: val_accuracy improved from 0.83750 to 0.83875, saving model to results/vectors/nasNet/fold_2.h5\n",
            "Epoch 181/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2463 - accuracy: 1.0000 - val_loss: 0.7349 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.83875\n",
            "Epoch 182/200\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 0.2430 - accuracy: 1.0000 - val_loss: 0.7194 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.83875\n",
            "Epoch 183/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2409 - accuracy: 1.0000 - val_loss: 0.7230 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.83875\n",
            "Epoch 184/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2391 - accuracy: 1.0000 - val_loss: 0.7164 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.83875\n",
            "Epoch 185/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2377 - accuracy: 1.0000 - val_loss: 0.7157 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.83875\n",
            "Epoch 186/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2348 - accuracy: 1.0000 - val_loss: 0.7155 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.83875\n",
            "Epoch 187/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2344 - accuracy: 0.9994 - val_loss: 0.7159 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.83875\n",
            "Epoch 188/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2301 - accuracy: 1.0000 - val_loss: 0.7233 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.83875\n",
            "Epoch 189/200\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.2285 - accuracy: 1.0000 - val_loss: 0.7450 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.83875\n",
            "Epoch 190/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2292 - accuracy: 1.0000 - val_loss: 0.7267 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.83875\n",
            "Epoch 191/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2250 - accuracy: 0.9997 - val_loss: 0.7184 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.83875\n",
            "Epoch 192/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2227 - accuracy: 1.0000 - val_loss: 0.7094 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.83875\n",
            "Epoch 193/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2217 - accuracy: 1.0000 - val_loss: 0.7610 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.83875\n",
            "Epoch 194/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2221 - accuracy: 1.0000 - val_loss: 0.7132 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.83875\n",
            "Epoch 195/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2174 - accuracy: 1.0000 - val_loss: 0.7066 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.83875\n",
            "Epoch 196/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2151 - accuracy: 1.0000 - val_loss: 0.7122 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.83875\n",
            "Epoch 197/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2128 - accuracy: 1.0000 - val_loss: 0.6958 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.83875\n",
            "Epoch 198/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2119 - accuracy: 1.0000 - val_loss: 0.7073 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.83875\n",
            "Epoch 199/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2093 - accuracy: 1.0000 - val_loss: 0.7079 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.83875\n",
            "Epoch 200/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2091 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.83875\n",
            "########################################################################\n",
            "Fold #3\n",
            "########################################################################\n",
            "Epoch 1/200\n",
            "100/100 [==============================] - 58s 269ms/step - loss: 10.6398 - accuracy: 0.6344 - val_loss: 10.2034 - val_accuracy: 0.7487\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.74875, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 2/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 10.0397 - accuracy: 0.7247 - val_loss: 9.7614 - val_accuracy: 0.7600\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.74875 to 0.76000, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 3/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 9.5347 - accuracy: 0.7831 - val_loss: 9.3436 - val_accuracy: 0.7812\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.76000 to 0.78125, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 4/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 9.1181 - accuracy: 0.7969 - val_loss: 8.9670 - val_accuracy: 0.7850\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.78125 to 0.78500, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 5/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 8.7244 - accuracy: 0.8212 - val_loss: 8.6190 - val_accuracy: 0.7925\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.78500 to 0.79250, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 6/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 8.3596 - accuracy: 0.8459 - val_loss: 8.2815 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.79250 to 0.80000, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 7/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 8.0168 - accuracy: 0.8603 - val_loss: 7.9738 - val_accuracy: 0.8012\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.80000 to 0.80125, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 8/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 7.6980 - accuracy: 0.8744 - val_loss: 7.6875 - val_accuracy: 0.8012\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.80125\n",
            "Epoch 9/200\n",
            "100/100 [==============================] - 6s 63ms/step - loss: 7.3917 - accuracy: 0.8881 - val_loss: 7.4138 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.80125 to 0.80625, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 10/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 7.1115 - accuracy: 0.8903 - val_loss: 7.1547 - val_accuracy: 0.8037\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.80625\n",
            "Epoch 11/200\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 6.8396 - accuracy: 0.9091 - val_loss: 6.9058 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.80625 to 0.81125, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 12/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 6.5910 - accuracy: 0.9187 - val_loss: 6.6891 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.81125\n",
            "Epoch 13/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 6.3564 - accuracy: 0.9191 - val_loss: 6.4569 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.81125\n",
            "Epoch 14/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 6.1296 - accuracy: 0.9294 - val_loss: 6.2546 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.81125 to 0.81625, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 5.9096 - accuracy: 0.9437 - val_loss: 6.0555 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.81625\n",
            "Epoch 16/200\n",
            "100/100 [==============================] - 6s 64ms/step - loss: 5.7091 - accuracy: 0.9484 - val_loss: 5.8785 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.81625 to 0.82000, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 17/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 5.5250 - accuracy: 0.9509 - val_loss: 5.7044 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.82000\n",
            "Epoch 18/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 5.3417 - accuracy: 0.9559 - val_loss: 5.5305 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.82000\n",
            "Epoch 19/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 5.1755 - accuracy: 0.9587 - val_loss: 5.3744 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.82000 to 0.82125, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 20/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 5.0127 - accuracy: 0.9603 - val_loss: 5.2271 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.82125 to 0.82375, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 21/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 4.8556 - accuracy: 0.9656 - val_loss: 5.0850 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.82375\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - 7s 69ms/step - loss: 4.7009 - accuracy: 0.9747 - val_loss: 4.9507 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.82375\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 4.5697 - accuracy: 0.9778 - val_loss: 4.8152 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.82375 to 0.83000, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 24/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 4.4270 - accuracy: 0.9787 - val_loss: 4.6905 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.83000\n",
            "Epoch 25/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 4.3009 - accuracy: 0.9803 - val_loss: 4.5792 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.83000\n",
            "Epoch 26/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 4.1784 - accuracy: 0.9825 - val_loss: 4.4633 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.83000\n",
            "Epoch 27/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 4.0629 - accuracy: 0.9812 - val_loss: 4.3510 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.83000\n",
            "Epoch 28/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.9468 - accuracy: 0.9853 - val_loss: 4.2450 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.83000\n",
            "Epoch 29/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.8385 - accuracy: 0.9859 - val_loss: 4.1406 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.83000\n",
            "Epoch 30/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.7318 - accuracy: 0.9887 - val_loss: 4.0417 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.83000\n",
            "Epoch 31/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.6319 - accuracy: 0.9872 - val_loss: 3.9517 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.83000\n",
            "Epoch 32/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.5331 - accuracy: 0.9894 - val_loss: 3.8586 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.83000\n",
            "Epoch 33/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.4359 - accuracy: 0.9934 - val_loss: 3.7769 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.83000\n",
            "Epoch 34/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 3.3474 - accuracy: 0.9934 - val_loss: 3.6814 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.83000 to 0.83375, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.2573 - accuracy: 0.9928 - val_loss: 3.6014 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.83375\n",
            "Epoch 36/200\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 3.1709 - accuracy: 0.9922 - val_loss: 3.5130 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.83375\n",
            "Epoch 37/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 3.0866 - accuracy: 0.9934 - val_loss: 3.4449 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.83375\n",
            "Epoch 38/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.0099 - accuracy: 0.9934 - val_loss: 3.3663 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.83375\n",
            "Epoch 39/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.9305 - accuracy: 0.9947 - val_loss: 3.2946 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.83375\n",
            "Epoch 40/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.8531 - accuracy: 0.9959 - val_loss: 3.2232 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.83375\n",
            "Epoch 41/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.7767 - accuracy: 0.9962 - val_loss: 3.1544 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.83375\n",
            "Epoch 42/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 2.7057 - accuracy: 0.9972 - val_loss: 3.0838 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.83375\n",
            "Epoch 43/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.6383 - accuracy: 0.9966 - val_loss: 3.0382 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.83375\n",
            "Epoch 44/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.5713 - accuracy: 0.9975 - val_loss: 2.9633 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.83375\n",
            "Epoch 45/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.5037 - accuracy: 0.9972 - val_loss: 2.9008 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.83375\n",
            "Epoch 46/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.4428 - accuracy: 0.9966 - val_loss: 2.8455 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.83375\n",
            "Epoch 47/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 2.3776 - accuracy: 0.9972 - val_loss: 2.7707 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.83375 to 0.83500, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 48/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.3202 - accuracy: 0.9969 - val_loss: 2.7091 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.83500\n",
            "Epoch 49/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 2.2596 - accuracy: 0.9987 - val_loss: 2.6670 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.83500\n",
            "Epoch 50/200\n",
            "100/100 [==============================] - 6s 61ms/step - loss: 2.2017 - accuracy: 0.9994 - val_loss: 2.6137 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00050: val_accuracy improved from 0.83500 to 0.83875, saving model to results/vectors/nasNet/fold_3.h5\n",
            "Epoch 51/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 2.1472 - accuracy: 0.9978 - val_loss: 2.5612 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.83875\n",
            "Epoch 52/200\n",
            "100/100 [==============================] - 6s 63ms/step - loss: 2.0942 - accuracy: 0.9978 - val_loss: 2.5041 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.83875\n",
            "Epoch 53/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.0399 - accuracy: 0.9969 - val_loss: 2.4589 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.83875\n",
            "Epoch 54/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.9891 - accuracy: 0.9991 - val_loss: 2.4111 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.83875\n",
            "Epoch 55/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.9388 - accuracy: 0.9984 - val_loss: 2.3672 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.83875\n",
            "Epoch 56/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.8916 - accuracy: 0.9978 - val_loss: 2.3198 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.83875\n",
            "Epoch 57/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.8468 - accuracy: 0.9987 - val_loss: 2.2749 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.83875\n",
            "Epoch 58/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.8005 - accuracy: 0.9987 - val_loss: 2.2310 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.83875\n",
            "Epoch 59/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.7541 - accuracy: 0.9987 - val_loss: 2.1878 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.83875\n",
            "Epoch 60/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.7132 - accuracy: 0.9994 - val_loss: 2.1421 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.83875\n",
            "Epoch 61/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.6696 - accuracy: 0.9994 - val_loss: 2.1139 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.83875\n",
            "Epoch 62/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.6318 - accuracy: 0.9991 - val_loss: 2.0689 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.83875\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.5910 - accuracy: 0.9987 - val_loss: 2.0418 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.83875\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.5538 - accuracy: 0.9987 - val_loss: 2.0192 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.83875\n",
            "Epoch 65/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.5154 - accuracy: 0.9991 - val_loss: 1.9612 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.83875\n",
            "Epoch 66/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.4792 - accuracy: 0.9997 - val_loss: 1.9331 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.83875\n",
            "Epoch 67/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.4443 - accuracy: 0.9994 - val_loss: 1.9094 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.83875\n",
            "Epoch 68/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.4092 - accuracy: 0.9991 - val_loss: 1.8663 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.83875\n",
            "Epoch 69/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.3793 - accuracy: 0.9997 - val_loss: 1.8391 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.83875\n",
            "Epoch 70/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.3449 - accuracy: 0.9994 - val_loss: 1.8124 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.83875\n",
            "Epoch 71/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.3148 - accuracy: 0.9991 - val_loss: 1.7716 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.83875\n",
            "Epoch 72/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.2841 - accuracy: 0.9994 - val_loss: 1.7536 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.83875\n",
            "Epoch 73/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.2543 - accuracy: 0.9994 - val_loss: 1.7321 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.83875\n",
            "Epoch 74/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.2274 - accuracy: 0.9994 - val_loss: 1.6921 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.83875\n",
            "Epoch 75/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.1989 - accuracy: 0.9997 - val_loss: 1.6643 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.83875\n",
            "Epoch 76/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.1704 - accuracy: 0.9987 - val_loss: 1.6441 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.83875\n",
            "Epoch 77/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.1442 - accuracy: 0.9991 - val_loss: 1.6181 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.83875\n",
            "Epoch 78/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.1201 - accuracy: 0.9987 - val_loss: 1.5896 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.83875\n",
            "Epoch 79/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0929 - accuracy: 0.9997 - val_loss: 1.5642 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.83875\n",
            "Epoch 80/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.0711 - accuracy: 0.9994 - val_loss: 1.5376 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.83875\n",
            "Epoch 81/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0469 - accuracy: 0.9991 - val_loss: 1.5264 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.83875\n",
            "Epoch 82/200\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 1.0245 - accuracy: 0.9994 - val_loss: 1.5136 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.83875\n",
            "Epoch 83/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0016 - accuracy: 1.0000 - val_loss: 1.4858 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.83875\n",
            "Epoch 84/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.9809 - accuracy: 0.9997 - val_loss: 1.4668 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.83875\n",
            "Epoch 85/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.9605 - accuracy: 0.9997 - val_loss: 1.4400 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.83875\n",
            "Epoch 86/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.9393 - accuracy: 1.0000 - val_loss: 1.4181 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.83875\n",
            "Epoch 87/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.9208 - accuracy: 0.9994 - val_loss: 1.4159 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.83875\n",
            "Epoch 88/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.9018 - accuracy: 0.9997 - val_loss: 1.3912 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.83875\n",
            "Epoch 89/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8829 - accuracy: 0.9997 - val_loss: 1.3821 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.83875\n",
            "Epoch 90/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8662 - accuracy: 0.9994 - val_loss: 1.3473 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.83875\n",
            "Epoch 91/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8467 - accuracy: 1.0000 - val_loss: 1.3378 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.83875\n",
            "Epoch 92/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.8316 - accuracy: 0.9997 - val_loss: 1.3263 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.83875\n",
            "Epoch 93/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.8126 - accuracy: 1.0000 - val_loss: 1.3062 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.83875\n",
            "Epoch 94/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7973 - accuracy: 1.0000 - val_loss: 1.2837 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.83875\n",
            "Epoch 95/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7806 - accuracy: 0.9997 - val_loss: 1.2697 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.83875\n",
            "Epoch 96/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7659 - accuracy: 1.0000 - val_loss: 1.2636 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.83875\n",
            "Epoch 97/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7515 - accuracy: 0.9994 - val_loss: 1.2606 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.83875\n",
            "Epoch 98/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7382 - accuracy: 0.9994 - val_loss: 1.2375 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.83875\n",
            "Epoch 99/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7229 - accuracy: 0.9997 - val_loss: 1.2147 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.83875\n",
            "Epoch 100/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7100 - accuracy: 0.9994 - val_loss: 1.2105 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.83875\n",
            "Epoch 101/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6976 - accuracy: 0.9994 - val_loss: 1.2047 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.83875\n",
            "Epoch 102/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6855 - accuracy: 1.0000 - val_loss: 1.1760 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.83875\n",
            "Epoch 103/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.6703 - accuracy: 0.9997 - val_loss: 1.1744 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.83875\n",
            "Epoch 104/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.6612 - accuracy: 1.0000 - val_loss: 1.1664 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.83875\n",
            "Epoch 105/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6513 - accuracy: 1.0000 - val_loss: 1.1613 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.83875\n",
            "Epoch 106/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.6368 - accuracy: 0.9997 - val_loss: 1.1628 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.83875\n",
            "Epoch 107/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6267 - accuracy: 0.9997 - val_loss: 1.1335 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.83875\n",
            "Epoch 108/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6150 - accuracy: 0.9997 - val_loss: 1.1149 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.83875\n",
            "Epoch 109/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6043 - accuracy: 1.0000 - val_loss: 1.1182 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.83875\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5944 - accuracy: 0.9994 - val_loss: 1.1136 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.83875\n",
            "Epoch 111/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5857 - accuracy: 0.9994 - val_loss: 1.0913 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.83875\n",
            "Epoch 112/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5749 - accuracy: 1.0000 - val_loss: 1.1019 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.83875\n",
            "Epoch 113/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5662 - accuracy: 0.9997 - val_loss: 1.0782 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.83875\n",
            "Epoch 114/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5564 - accuracy: 1.0000 - val_loss: 1.0836 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.83875\n",
            "Epoch 115/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5484 - accuracy: 0.9997 - val_loss: 1.0577 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.83875\n",
            "Epoch 116/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5377 - accuracy: 1.0000 - val_loss: 1.0548 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.83875\n",
            "Epoch 117/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5300 - accuracy: 0.9997 - val_loss: 1.0446 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.83875\n",
            "Epoch 118/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5237 - accuracy: 1.0000 - val_loss: 1.0415 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.83875\n",
            "Epoch 119/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5134 - accuracy: 0.9994 - val_loss: 1.0296 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.83875\n",
            "Epoch 120/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5058 - accuracy: 0.9997 - val_loss: 1.0259 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.83875\n",
            "Epoch 121/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4964 - accuracy: 1.0000 - val_loss: 1.0191 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.83875\n",
            "Epoch 122/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4903 - accuracy: 0.9994 - val_loss: 1.0046 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.83875\n",
            "Epoch 123/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4819 - accuracy: 0.9997 - val_loss: 1.0000 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.83875\n",
            "Epoch 124/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4758 - accuracy: 0.9994 - val_loss: 0.9907 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.83875\n",
            "Epoch 125/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4686 - accuracy: 0.9997 - val_loss: 0.9920 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.83875\n",
            "Epoch 126/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4617 - accuracy: 0.9997 - val_loss: 0.9631 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.83875\n",
            "Epoch 127/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4532 - accuracy: 1.0000 - val_loss: 0.9774 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.83875\n",
            "Epoch 128/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4472 - accuracy: 1.0000 - val_loss: 0.9635 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.83875\n",
            "Epoch 129/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 0.4401 - accuracy: 1.0000 - val_loss: 0.9647 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.83875\n",
            "Epoch 130/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4342 - accuracy: 1.0000 - val_loss: 0.9584 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.83875\n",
            "Epoch 131/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4282 - accuracy: 0.9997 - val_loss: 0.9527 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.83875\n",
            "Epoch 132/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4240 - accuracy: 1.0000 - val_loss: 0.9457 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.83875\n",
            "Epoch 133/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4153 - accuracy: 1.0000 - val_loss: 0.9410 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.83875\n",
            "Epoch 134/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4105 - accuracy: 1.0000 - val_loss: 0.9438 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.83875\n",
            "Epoch 135/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4047 - accuracy: 1.0000 - val_loss: 0.9322 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.83875\n",
            "Epoch 136/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3992 - accuracy: 0.9997 - val_loss: 0.9301 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.83875\n",
            "Epoch 137/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3938 - accuracy: 1.0000 - val_loss: 0.9174 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.83875\n",
            "Epoch 138/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3886 - accuracy: 1.0000 - val_loss: 0.9178 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.83875\n",
            "Epoch 139/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3819 - accuracy: 1.0000 - val_loss: 0.9062 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.83875\n",
            "Epoch 140/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3767 - accuracy: 1.0000 - val_loss: 0.8989 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.83875\n",
            "Epoch 141/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3728 - accuracy: 0.9994 - val_loss: 0.8973 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.83875\n",
            "Epoch 142/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3686 - accuracy: 0.9997 - val_loss: 0.8982 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.83875\n",
            "Epoch 143/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3620 - accuracy: 1.0000 - val_loss: 0.8967 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.83875\n",
            "Epoch 144/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3593 - accuracy: 0.9997 - val_loss: 0.9073 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.83875\n",
            "Epoch 145/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3558 - accuracy: 1.0000 - val_loss: 0.8774 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.83875\n",
            "Epoch 146/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3488 - accuracy: 1.0000 - val_loss: 0.8799 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.83875\n",
            "Epoch 147/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3455 - accuracy: 0.9997 - val_loss: 0.8770 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.83875\n",
            "Epoch 148/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3402 - accuracy: 1.0000 - val_loss: 0.8747 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.83875\n",
            "Epoch 149/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3386 - accuracy: 0.9994 - val_loss: 0.8772 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.83875\n",
            "Epoch 150/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3345 - accuracy: 1.0000 - val_loss: 0.8554 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.83875\n",
            "Epoch 151/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3300 - accuracy: 0.9997 - val_loss: 0.8647 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.83875\n",
            "Epoch 152/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3243 - accuracy: 1.0000 - val_loss: 0.8477 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.83875\n",
            "Epoch 153/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3209 - accuracy: 1.0000 - val_loss: 0.8635 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.83875\n",
            "Epoch 154/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3173 - accuracy: 1.0000 - val_loss: 0.8829 - val_accuracy: 0.7962\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.83875\n",
            "Epoch 155/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.3143 - accuracy: 0.9997 - val_loss: 0.8428 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.83875\n",
            "Epoch 156/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3112 - accuracy: 1.0000 - val_loss: 0.8451 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.83875\n",
            "Epoch 157/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3071 - accuracy: 1.0000 - val_loss: 0.8524 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.83875\n",
            "Epoch 158/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3032 - accuracy: 1.0000 - val_loss: 0.8518 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.83875\n",
            "Epoch 159/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2990 - accuracy: 1.0000 - val_loss: 0.8479 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.83875\n",
            "Epoch 160/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2964 - accuracy: 1.0000 - val_loss: 0.8456 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.83875\n",
            "Epoch 161/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2939 - accuracy: 1.0000 - val_loss: 0.8260 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.83875\n",
            "Epoch 162/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2891 - accuracy: 1.0000 - val_loss: 0.8294 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.83875\n",
            "Epoch 163/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2865 - accuracy: 1.0000 - val_loss: 0.8441 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.83875\n",
            "Epoch 164/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.2836 - accuracy: 0.9997 - val_loss: 0.8142 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.83875\n",
            "Epoch 165/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.2795 - accuracy: 1.0000 - val_loss: 0.8230 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.83875\n",
            "Epoch 166/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.2781 - accuracy: 1.0000 - val_loss: 0.8251 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.83875\n",
            "Epoch 167/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.2747 - accuracy: 1.0000 - val_loss: 0.8424 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.83875\n",
            "Epoch 168/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.2709 - accuracy: 1.0000 - val_loss: 0.8061 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.83875\n",
            "Epoch 169/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.2684 - accuracy: 1.0000 - val_loss: 0.8400 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.83875\n",
            "Epoch 170/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2658 - accuracy: 1.0000 - val_loss: 0.8170 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.83875\n",
            "Epoch 171/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2632 - accuracy: 1.0000 - val_loss: 0.8006 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.83875\n",
            "Epoch 172/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2601 - accuracy: 1.0000 - val_loss: 0.8041 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.83875\n",
            "Epoch 173/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2587 - accuracy: 1.0000 - val_loss: 0.8563 - val_accuracy: 0.7937\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.83875\n",
            "Epoch 174/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2580 - accuracy: 0.9997 - val_loss: 0.7883 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.83875\n",
            "Epoch 175/200\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 0.2538 - accuracy: 1.0000 - val_loss: 0.7958 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.83875\n",
            "Epoch 176/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2508 - accuracy: 1.0000 - val_loss: 0.8210 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.83875\n",
            "Epoch 177/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2478 - accuracy: 1.0000 - val_loss: 0.7972 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.83875\n",
            "Epoch 178/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2448 - accuracy: 1.0000 - val_loss: 0.7905 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.83875\n",
            "Epoch 179/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2421 - accuracy: 1.0000 - val_loss: 0.7874 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.83875\n",
            "Epoch 180/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2400 - accuracy: 1.0000 - val_loss: 0.7898 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.83875\n",
            "Epoch 181/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2384 - accuracy: 1.0000 - val_loss: 0.8321 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.83875\n",
            "Epoch 182/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2376 - accuracy: 1.0000 - val_loss: 0.7838 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.83875\n",
            "Epoch 183/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2343 - accuracy: 1.0000 - val_loss: 0.8152 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.83875\n",
            "Epoch 184/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.2321 - accuracy: 1.0000 - val_loss: 0.8071 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.83875\n",
            "Epoch 185/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2326 - accuracy: 1.0000 - val_loss: 0.7773 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.83875\n",
            "Epoch 186/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2268 - accuracy: 1.0000 - val_loss: 0.7796 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.83875\n",
            "Epoch 187/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2251 - accuracy: 1.0000 - val_loss: 0.7761 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.83875\n",
            "Epoch 188/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2234 - accuracy: 1.0000 - val_loss: 0.7622 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.83875\n",
            "Epoch 189/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2219 - accuracy: 1.0000 - val_loss: 0.7671 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.83875\n",
            "Epoch 190/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2197 - accuracy: 1.0000 - val_loss: 0.7646 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.83875\n",
            "Epoch 191/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2164 - accuracy: 1.0000 - val_loss: 0.7682 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.83875\n",
            "Epoch 192/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2155 - accuracy: 1.0000 - val_loss: 0.7669 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.83875\n",
            "Epoch 193/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2153 - accuracy: 1.0000 - val_loss: 0.7621 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.83875\n",
            "Epoch 194/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2115 - accuracy: 1.0000 - val_loss: 0.7799 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.83875\n",
            "Epoch 195/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2118 - accuracy: 1.0000 - val_loss: 0.7562 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.83875\n",
            "Epoch 196/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2089 - accuracy: 0.9997 - val_loss: 0.7649 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.83875\n",
            "Epoch 197/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2067 - accuracy: 1.0000 - val_loss: 0.7823 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.83875\n",
            "Epoch 198/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2063 - accuracy: 0.9997 - val_loss: 0.7560 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.83875\n",
            "Epoch 199/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.2033 - accuracy: 1.0000 - val_loss: 0.7695 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.83875\n",
            "Epoch 200/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2018 - accuracy: 1.0000 - val_loss: 0.7526 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.83875\n",
            "########################################################################\n",
            "Fold #4\n",
            "########################################################################\n",
            "Epoch 1/200\n",
            "100/100 [==============================] - 57s 260ms/step - loss: 10.6667 - accuracy: 0.6272 - val_loss: 10.1974 - val_accuracy: 0.7337\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.73375, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 2/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 10.0236 - accuracy: 0.7391 - val_loss: 9.7450 - val_accuracy: 0.7750\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.73375 to 0.77500, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 3/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 9.5662 - accuracy: 0.7703 - val_loss: 9.3563 - val_accuracy: 0.7688\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.77500\n",
            "Epoch 4/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 9.1356 - accuracy: 0.8031 - val_loss: 8.9872 - val_accuracy: 0.7850\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.77500 to 0.78500, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 5/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 8.7613 - accuracy: 0.8241 - val_loss: 8.6464 - val_accuracy: 0.7825\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.78500\n",
            "Epoch 6/200\n",
            "100/100 [==============================] - 6s 61ms/step - loss: 8.3980 - accuracy: 0.8487 - val_loss: 8.3129 - val_accuracy: 0.7975\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.78500 to 0.79750, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 7/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 8.0693 - accuracy: 0.8550 - val_loss: 8.0077 - val_accuracy: 0.7900\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.79750\n",
            "Epoch 8/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 7.7507 - accuracy: 0.8731 - val_loss: 7.7147 - val_accuracy: 0.7962\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.79750\n",
            "Epoch 9/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 7.4586 - accuracy: 0.8822 - val_loss: 7.4565 - val_accuracy: 0.8012\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.79750 to 0.80125, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 10/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 7.1819 - accuracy: 0.8903 - val_loss: 7.1963 - val_accuracy: 0.8012\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.80125\n",
            "Epoch 11/200\n",
            "100/100 [==============================] - 6s 64ms/step - loss: 6.9099 - accuracy: 0.9081 - val_loss: 6.9496 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.80125 to 0.80500, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 12/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 6.6644 - accuracy: 0.9141 - val_loss: 6.7259 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.80500 to 0.80750, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 13/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 6.4342 - accuracy: 0.9256 - val_loss: 6.5073 - val_accuracy: 0.7975\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.80750\n",
            "Epoch 14/200\n",
            "100/100 [==============================] - 6s 64ms/step - loss: 6.2244 - accuracy: 0.9250 - val_loss: 6.3013 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.80750 to 0.81750, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 6.0072 - accuracy: 0.9366 - val_loss: 6.1094 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.81750\n",
            "Epoch 16/200\n",
            "100/100 [==============================] - 6s 64ms/step - loss: 5.8156 - accuracy: 0.9341 - val_loss: 5.9322 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.81750 to 0.82125, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 17/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 5.6255 - accuracy: 0.9459 - val_loss: 5.7553 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.82125\n",
            "Epoch 18/200\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 5.4485 - accuracy: 0.9537 - val_loss: 5.5921 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.82125\n",
            "Epoch 19/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 5.2780 - accuracy: 0.9606 - val_loss: 5.4352 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.82125 to 0.82500, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 20/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 5.1155 - accuracy: 0.9656 - val_loss: 5.2847 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.82500\n",
            "Epoch 21/200\n",
            "100/100 [==============================] - 6s 65ms/step - loss: 4.9682 - accuracy: 0.9669 - val_loss: 5.1432 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.82500\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 4.8208 - accuracy: 0.9675 - val_loss: 5.0279 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.82500\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 4.6782 - accuracy: 0.9712 - val_loss: 4.8847 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.82500\n",
            "Epoch 24/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 4.5453 - accuracy: 0.9775 - val_loss: 4.7533 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.82500\n",
            "Epoch 25/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 4.4170 - accuracy: 0.9772 - val_loss: 4.6323 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.82500\n",
            "Epoch 26/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 4.2960 - accuracy: 0.9784 - val_loss: 4.5109 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.82500 to 0.82750, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 27/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 4.1775 - accuracy: 0.9806 - val_loss: 4.4011 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.82750\n",
            "Epoch 28/200\n",
            "100/100 [==============================] - 6s 64ms/step - loss: 4.0629 - accuracy: 0.9806 - val_loss: 4.2995 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.82750\n",
            "Epoch 29/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 3.9561 - accuracy: 0.9841 - val_loss: 4.1948 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.82750\n",
            "Epoch 30/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.8478 - accuracy: 0.9869 - val_loss: 4.0942 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.82750 to 0.82875, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 31/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 3.7494 - accuracy: 0.9869 - val_loss: 3.9929 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.82875\n",
            "Epoch 32/200\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 3.6501 - accuracy: 0.9875 - val_loss: 3.8986 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.82875 to 0.83125, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 33/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.5511 - accuracy: 0.9909 - val_loss: 3.8157 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.83125\n",
            "Epoch 34/200\n",
            "100/100 [==============================] - 6s 65ms/step - loss: 3.4586 - accuracy: 0.9922 - val_loss: 3.7169 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.83125\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 3.3702 - accuracy: 0.9919 - val_loss: 3.6351 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.83125 to 0.84000, saving model to results/vectors/nasNet/fold_4.h5\n",
            "Epoch 36/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.2833 - accuracy: 0.9928 - val_loss: 3.5546 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.84000\n",
            "Epoch 37/200\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 3.2002 - accuracy: 0.9944 - val_loss: 3.4796 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.84000\n",
            "Epoch 38/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 3.1159 - accuracy: 0.9912 - val_loss: 3.3953 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.84000\n",
            "Epoch 39/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.0359 - accuracy: 0.9947 - val_loss: 3.3333 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.84000\n",
            "Epoch 40/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.9591 - accuracy: 0.9947 - val_loss: 3.2558 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.84000\n",
            "Epoch 41/200\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 2.8819 - accuracy: 0.9966 - val_loss: 3.1676 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.84000\n",
            "Epoch 42/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.8123 - accuracy: 0.9950 - val_loss: 3.1038 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.84000\n",
            "Epoch 43/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.7414 - accuracy: 0.9972 - val_loss: 3.0343 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.84000\n",
            "Epoch 44/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.6720 - accuracy: 0.9962 - val_loss: 2.9708 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.84000\n",
            "Epoch 45/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.6021 - accuracy: 0.9978 - val_loss: 2.9156 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.84000\n",
            "Epoch 46/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.5377 - accuracy: 0.9969 - val_loss: 2.8500 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.84000\n",
            "Epoch 47/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.4751 - accuracy: 0.9959 - val_loss: 2.7842 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.84000\n",
            "Epoch 48/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.4117 - accuracy: 0.9981 - val_loss: 2.7261 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.84000\n",
            "Epoch 49/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 2.3512 - accuracy: 0.9972 - val_loss: 2.6717 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.84000\n",
            "Epoch 50/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.2939 - accuracy: 0.9972 - val_loss: 2.6152 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.84000\n",
            "Epoch 51/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.2358 - accuracy: 0.9975 - val_loss: 2.5643 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.84000\n",
            "Epoch 52/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.1817 - accuracy: 0.9978 - val_loss: 2.5083 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.84000\n",
            "Epoch 53/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.1255 - accuracy: 0.9991 - val_loss: 2.4586 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.84000\n",
            "Epoch 54/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.0758 - accuracy: 0.9978 - val_loss: 2.4090 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.84000\n",
            "Epoch 55/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.0231 - accuracy: 0.9984 - val_loss: 2.3591 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.84000\n",
            "Epoch 56/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.9728 - accuracy: 0.9991 - val_loss: 2.3099 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.84000\n",
            "Epoch 57/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.9259 - accuracy: 0.9987 - val_loss: 2.2642 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.84000\n",
            "Epoch 58/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.8777 - accuracy: 0.9994 - val_loss: 2.2128 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.84000\n",
            "Epoch 59/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.8312 - accuracy: 0.9997 - val_loss: 2.1746 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.84000\n",
            "Epoch 60/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.7877 - accuracy: 0.9991 - val_loss: 2.1270 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.84000\n",
            "Epoch 61/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.7440 - accuracy: 0.9984 - val_loss: 2.0883 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.84000\n",
            "Epoch 62/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.7017 - accuracy: 0.9994 - val_loss: 2.0462 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.84000\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.6606 - accuracy: 0.9994 - val_loss: 2.0102 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.84000\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.6220 - accuracy: 0.9994 - val_loss: 1.9747 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.84000\n",
            "Epoch 65/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.5822 - accuracy: 0.9994 - val_loss: 1.9330 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.84000\n",
            "Epoch 66/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 1.5452 - accuracy: 0.9991 - val_loss: 1.8990 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.84000\n",
            "Epoch 67/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.5072 - accuracy: 0.9994 - val_loss: 1.8667 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.84000\n",
            "Epoch 68/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.4742 - accuracy: 0.9987 - val_loss: 1.8489 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.84000\n",
            "Epoch 69/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.4375 - accuracy: 0.9997 - val_loss: 1.7953 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.84000\n",
            "Epoch 70/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.4044 - accuracy: 0.9994 - val_loss: 1.7711 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.84000\n",
            "Epoch 71/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.3716 - accuracy: 1.0000 - val_loss: 1.7402 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.84000\n",
            "Epoch 72/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.3391 - accuracy: 1.0000 - val_loss: 1.6954 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.84000\n",
            "Epoch 73/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.3089 - accuracy: 0.9997 - val_loss: 1.6843 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.84000\n",
            "Epoch 74/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.2801 - accuracy: 0.9991 - val_loss: 1.6458 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.84000\n",
            "Epoch 75/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.2470 - accuracy: 0.9994 - val_loss: 1.6295 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.84000\n",
            "Epoch 76/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.2217 - accuracy: 0.9994 - val_loss: 1.5867 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.84000\n",
            "Epoch 77/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.1935 - accuracy: 0.9997 - val_loss: 1.5643 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.84000\n",
            "Epoch 78/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.1652 - accuracy: 1.0000 - val_loss: 1.5406 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.84000\n",
            "Epoch 79/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.1396 - accuracy: 0.9997 - val_loss: 1.5115 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.84000\n",
            "Epoch 80/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.1164 - accuracy: 0.9997 - val_loss: 1.4990 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.84000\n",
            "Epoch 81/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.0900 - accuracy: 1.0000 - val_loss: 1.4688 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.84000\n",
            "Epoch 82/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0667 - accuracy: 1.0000 - val_loss: 1.4368 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.84000\n",
            "Epoch 83/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0434 - accuracy: 0.9994 - val_loss: 1.4174 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.84000\n",
            "Epoch 84/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0216 - accuracy: 0.9991 - val_loss: 1.3969 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.84000\n",
            "Epoch 85/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.9995 - accuracy: 0.9997 - val_loss: 1.3763 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.84000\n",
            "Epoch 86/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.9767 - accuracy: 1.0000 - val_loss: 1.3682 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.84000\n",
            "Epoch 87/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.9575 - accuracy: 0.9994 - val_loss: 1.3448 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.84000\n",
            "Epoch 88/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.9385 - accuracy: 1.0000 - val_loss: 1.3135 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.84000\n",
            "Epoch 89/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.9165 - accuracy: 1.0000 - val_loss: 1.3029 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.84000\n",
            "Epoch 90/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.8996 - accuracy: 1.0000 - val_loss: 1.2816 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.84000\n",
            "Epoch 91/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.8803 - accuracy: 0.9997 - val_loss: 1.2749 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.84000\n",
            "Epoch 92/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.8619 - accuracy: 0.9997 - val_loss: 1.2682 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.84000\n",
            "Epoch 93/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.8458 - accuracy: 0.9994 - val_loss: 1.2315 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.84000\n",
            "Epoch 94/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.8282 - accuracy: 0.9997 - val_loss: 1.2190 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.84000\n",
            "Epoch 95/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.8150 - accuracy: 0.9987 - val_loss: 1.2165 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.84000\n",
            "Epoch 96/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7961 - accuracy: 1.0000 - val_loss: 1.2009 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.84000\n",
            "Epoch 97/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7815 - accuracy: 1.0000 - val_loss: 1.1664 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.84000\n",
            "Epoch 98/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7656 - accuracy: 1.0000 - val_loss: 1.1530 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.84000\n",
            "Epoch 99/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7516 - accuracy: 1.0000 - val_loss: 1.1455 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.84000\n",
            "Epoch 100/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7375 - accuracy: 0.9994 - val_loss: 1.1247 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.84000\n",
            "Epoch 101/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7239 - accuracy: 1.0000 - val_loss: 1.1102 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.84000\n",
            "Epoch 102/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7104 - accuracy: 1.0000 - val_loss: 1.1017 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.84000\n",
            "Epoch 103/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6988 - accuracy: 0.9997 - val_loss: 1.0960 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.84000\n",
            "Epoch 104/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.6844 - accuracy: 0.9991 - val_loss: 1.0814 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.84000\n",
            "Epoch 105/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 0.6718 - accuracy: 1.0000 - val_loss: 1.0728 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.84000\n",
            "Epoch 106/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.6591 - accuracy: 1.0000 - val_loss: 1.0651 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.84000\n",
            "Epoch 107/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.6491 - accuracy: 0.9997 - val_loss: 1.0479 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.84000\n",
            "Epoch 108/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6376 - accuracy: 1.0000 - val_loss: 1.0440 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.84000\n",
            "Epoch 109/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.6260 - accuracy: 1.0000 - val_loss: 1.0245 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.84000\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.6158 - accuracy: 0.9997 - val_loss: 1.0163 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.84000\n",
            "Epoch 111/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.6042 - accuracy: 1.0000 - val_loss: 1.0095 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.84000\n",
            "Epoch 112/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5953 - accuracy: 0.9997 - val_loss: 0.9934 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.84000\n",
            "Epoch 113/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.5848 - accuracy: 0.9997 - val_loss: 0.9994 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.84000\n",
            "Epoch 114/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.5739 - accuracy: 1.0000 - val_loss: 0.9869 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.84000\n",
            "Epoch 115/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.5650 - accuracy: 1.0000 - val_loss: 0.9819 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.84000\n",
            "Epoch 116/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5558 - accuracy: 0.9997 - val_loss: 0.9686 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.84000\n",
            "Epoch 117/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5458 - accuracy: 1.0000 - val_loss: 0.9504 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.84000\n",
            "Epoch 118/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.5382 - accuracy: 0.9997 - val_loss: 0.9406 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.84000\n",
            "Epoch 119/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.5291 - accuracy: 1.0000 - val_loss: 0.9316 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.84000\n",
            "Epoch 120/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5213 - accuracy: 0.9997 - val_loss: 0.9381 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.84000\n",
            "Epoch 121/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.5122 - accuracy: 0.9997 - val_loss: 0.9225 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.84000\n",
            "Epoch 122/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.5039 - accuracy: 1.0000 - val_loss: 0.9120 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.84000\n",
            "Epoch 123/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4964 - accuracy: 1.0000 - val_loss: 0.9005 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.84000\n",
            "Epoch 124/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4881 - accuracy: 1.0000 - val_loss: 0.8889 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.84000\n",
            "Epoch 125/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4823 - accuracy: 0.9997 - val_loss: 0.8785 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.84000\n",
            "Epoch 126/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4746 - accuracy: 1.0000 - val_loss: 0.8872 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.84000\n",
            "Epoch 127/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.4661 - accuracy: 1.0000 - val_loss: 0.8792 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.84000\n",
            "Epoch 128/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4593 - accuracy: 1.0000 - val_loss: 0.8705 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.84000\n",
            "Epoch 129/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4523 - accuracy: 1.0000 - val_loss: 0.8573 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.84000\n",
            "Epoch 130/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4455 - accuracy: 1.0000 - val_loss: 0.8635 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.84000\n",
            "Epoch 131/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.4389 - accuracy: 0.9997 - val_loss: 0.8443 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.84000\n",
            "Epoch 132/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4332 - accuracy: 1.0000 - val_loss: 0.8355 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.84000\n",
            "Epoch 133/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.4266 - accuracy: 1.0000 - val_loss: 0.8352 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.84000\n",
            "Epoch 134/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4197 - accuracy: 1.0000 - val_loss: 0.8328 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.84000\n",
            "Epoch 135/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4138 - accuracy: 1.0000 - val_loss: 0.8285 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.84000\n",
            "Epoch 136/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4084 - accuracy: 1.0000 - val_loss: 0.8123 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.84000\n",
            "Epoch 137/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4035 - accuracy: 0.9997 - val_loss: 0.8122 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.84000\n",
            "Epoch 138/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3974 - accuracy: 1.0000 - val_loss: 0.8060 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.84000\n",
            "Epoch 139/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3930 - accuracy: 1.0000 - val_loss: 0.8030 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.84000\n",
            "Epoch 140/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3854 - accuracy: 1.0000 - val_loss: 0.7995 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.84000\n",
            "Epoch 141/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3813 - accuracy: 0.9997 - val_loss: 0.7977 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.84000\n",
            "Epoch 142/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3774 - accuracy: 1.0000 - val_loss: 0.7817 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.84000\n",
            "Epoch 143/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3712 - accuracy: 0.9997 - val_loss: 0.7895 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.84000\n",
            "Epoch 144/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3671 - accuracy: 1.0000 - val_loss: 0.7830 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.84000\n",
            "Epoch 145/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3608 - accuracy: 1.0000 - val_loss: 0.7825 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.84000\n",
            "Epoch 146/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3575 - accuracy: 0.9997 - val_loss: 0.8083 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.84000\n",
            "Epoch 147/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.3535 - accuracy: 1.0000 - val_loss: 0.7767 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.84000\n",
            "Epoch 148/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3488 - accuracy: 0.9997 - val_loss: 0.7637 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.84000\n",
            "Epoch 149/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.3453 - accuracy: 1.0000 - val_loss: 0.7657 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.84000\n",
            "Epoch 150/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.3393 - accuracy: 1.0000 - val_loss: 0.7654 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.84000\n",
            "Epoch 151/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3355 - accuracy: 1.0000 - val_loss: 0.7570 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.84000\n",
            "Epoch 152/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3309 - accuracy: 1.0000 - val_loss: 0.7470 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.84000\n",
            "Epoch 153/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.7740 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.84000\n",
            "Epoch 154/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3243 - accuracy: 1.0000 - val_loss: 0.7443 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.84000\n",
            "Epoch 155/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.3202 - accuracy: 1.0000 - val_loss: 0.7344 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.84000\n",
            "Epoch 156/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3162 - accuracy: 1.0000 - val_loss: 0.7424 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.84000\n",
            "Epoch 157/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3124 - accuracy: 1.0000 - val_loss: 0.7354 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.84000\n",
            "Epoch 158/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3083 - accuracy: 1.0000 - val_loss: 0.7420 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.84000\n",
            "Epoch 159/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3055 - accuracy: 1.0000 - val_loss: 0.7289 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.84000\n",
            "Epoch 160/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3012 - accuracy: 1.0000 - val_loss: 0.7328 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.84000\n",
            "Epoch 161/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2978 - accuracy: 1.0000 - val_loss: 0.7362 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.84000\n",
            "Epoch 162/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 0.2950 - accuracy: 1.0000 - val_loss: 0.7176 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.84000\n",
            "Epoch 163/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2910 - accuracy: 1.0000 - val_loss: 0.7283 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.84000\n",
            "Epoch 164/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2900 - accuracy: 0.9997 - val_loss: 0.7169 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.84000\n",
            "Epoch 165/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2858 - accuracy: 1.0000 - val_loss: 0.7269 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.84000\n",
            "Epoch 166/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2858 - accuracy: 1.0000 - val_loss: 0.7152 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.84000\n",
            "Epoch 167/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2795 - accuracy: 0.9994 - val_loss: 0.7310 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.84000\n",
            "Epoch 168/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2767 - accuracy: 1.0000 - val_loss: 0.6980 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.84000\n",
            "Epoch 169/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.2736 - accuracy: 1.0000 - val_loss: 0.7069 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.84000\n",
            "Epoch 170/200\n",
            "100/100 [==============================] - 6s 65ms/step - loss: 0.2704 - accuracy: 1.0000 - val_loss: 0.6910 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.84000\n",
            "Epoch 171/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2682 - accuracy: 1.0000 - val_loss: 0.6998 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.84000\n",
            "Epoch 172/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2651 - accuracy: 1.0000 - val_loss: 0.6960 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.84000\n",
            "Epoch 173/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2643 - accuracy: 0.9994 - val_loss: 0.6792 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.84000\n",
            "Epoch 174/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2607 - accuracy: 0.9997 - val_loss: 0.6995 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.84000\n",
            "Epoch 175/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2571 - accuracy: 1.0000 - val_loss: 0.6852 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.84000\n",
            "Epoch 176/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2544 - accuracy: 0.9997 - val_loss: 0.6818 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.84000\n",
            "Epoch 177/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2526 - accuracy: 0.9997 - val_loss: 0.6807 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.84000\n",
            "Epoch 178/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2501 - accuracy: 1.0000 - val_loss: 0.7057 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.84000\n",
            "Epoch 179/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2488 - accuracy: 1.0000 - val_loss: 0.6948 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.84000\n",
            "Epoch 180/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2459 - accuracy: 0.9997 - val_loss: 0.6796 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.84000\n",
            "Epoch 181/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2440 - accuracy: 0.9994 - val_loss: 0.6718 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.84000\n",
            "Epoch 182/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2408 - accuracy: 1.0000 - val_loss: 0.6733 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.84000\n",
            "Epoch 183/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 0.2380 - accuracy: 0.9997 - val_loss: 0.6685 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.84000\n",
            "Epoch 184/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2376 - accuracy: 0.9997 - val_loss: 0.6685 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.84000\n",
            "Epoch 185/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2366 - accuracy: 0.9997 - val_loss: 0.6672 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.84000\n",
            "Epoch 186/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2331 - accuracy: 1.0000 - val_loss: 0.6728 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.84000\n",
            "Epoch 187/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2305 - accuracy: 1.0000 - val_loss: 0.6575 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.84000\n",
            "Epoch 188/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2281 - accuracy: 1.0000 - val_loss: 0.6592 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.84000\n",
            "Epoch 189/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2244 - accuracy: 1.0000 - val_loss: 0.6678 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.84000\n",
            "Epoch 190/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2241 - accuracy: 0.9997 - val_loss: 0.6538 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.84000\n",
            "Epoch 191/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.2229 - accuracy: 0.9997 - val_loss: 0.6613 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.84000\n",
            "Epoch 192/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2208 - accuracy: 1.0000 - val_loss: 0.6538 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.84000\n",
            "Epoch 193/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2181 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.84000\n",
            "Epoch 194/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2161 - accuracy: 1.0000 - val_loss: 0.6523 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.84000\n",
            "Epoch 195/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2153 - accuracy: 1.0000 - val_loss: 0.6436 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.84000\n",
            "Epoch 196/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2130 - accuracy: 1.0000 - val_loss: 0.6403 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.84000\n",
            "Epoch 197/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2102 - accuracy: 1.0000 - val_loss: 0.6340 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.84000\n",
            "Epoch 198/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2090 - accuracy: 1.0000 - val_loss: 0.6326 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.84000\n",
            "Epoch 199/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2062 - accuracy: 1.0000 - val_loss: 0.6355 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.84000\n",
            "Epoch 200/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2079 - accuracy: 0.9994 - val_loss: 0.6350 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.84000\n",
            "########################################################################\n",
            "Fold #5\n",
            "########################################################################\n",
            "Epoch 1/200\n",
            "100/100 [==============================] - 57s 268ms/step - loss: 10.6598 - accuracy: 0.6350 - val_loss: 10.2360 - val_accuracy: 0.7287\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.72875, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 2/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 10.0275 - accuracy: 0.7344 - val_loss: 9.7891 - val_accuracy: 0.7625\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.72875 to 0.76250, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 3/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 9.5510 - accuracy: 0.7734 - val_loss: 9.3896 - val_accuracy: 0.7625\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.76250\n",
            "Epoch 4/200\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 9.1308 - accuracy: 0.8069 - val_loss: 9.0077 - val_accuracy: 0.7775\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.76250 to 0.77750, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 5/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 8.7461 - accuracy: 0.8262 - val_loss: 8.6457 - val_accuracy: 0.7862\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.77750 to 0.78625, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 6/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 8.3772 - accuracy: 0.8431 - val_loss: 8.3206 - val_accuracy: 0.7825\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.78625\n",
            "Epoch 7/200\n",
            "100/100 [==============================] - 7s 69ms/step - loss: 8.0303 - accuracy: 0.8612 - val_loss: 8.0051 - val_accuracy: 0.7850\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.78625\n",
            "Epoch 8/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 7.7100 - accuracy: 0.8772 - val_loss: 7.7102 - val_accuracy: 0.7962\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.78625 to 0.79625, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 9/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 7.4078 - accuracy: 0.8856 - val_loss: 7.4331 - val_accuracy: 0.7862\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.79625\n",
            "Epoch 10/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 7.1213 - accuracy: 0.9059 - val_loss: 7.1634 - val_accuracy: 0.7950\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.79625\n",
            "Epoch 11/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 6.8527 - accuracy: 0.9103 - val_loss: 6.9238 - val_accuracy: 0.7937\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.79625\n",
            "Epoch 12/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 6.5985 - accuracy: 0.9187 - val_loss: 6.6869 - val_accuracy: 0.8087\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.79625 to 0.80875, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 13/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 6.3608 - accuracy: 0.9269 - val_loss: 6.4636 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.80875 to 0.81625, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 14/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 6.1434 - accuracy: 0.9256 - val_loss: 6.2599 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.81625\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 5.9262 - accuracy: 0.9356 - val_loss: 6.0632 - val_accuracy: 0.8100\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.81625\n",
            "Epoch 16/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 5.7273 - accuracy: 0.9425 - val_loss: 5.8816 - val_accuracy: 0.8112\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.81625\n",
            "Epoch 17/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 5.5380 - accuracy: 0.9497 - val_loss: 5.6961 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.81625 to 0.82875, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 18/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 5.3512 - accuracy: 0.9537 - val_loss: 5.5283 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.82875\n",
            "Epoch 19/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 5.1806 - accuracy: 0.9575 - val_loss: 5.3768 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.82875\n",
            "Epoch 20/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 5.0155 - accuracy: 0.9622 - val_loss: 5.2228 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.82875 to 0.83625, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 21/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 4.8677 - accuracy: 0.9628 - val_loss: 5.0755 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.83625\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - 6s 62ms/step - loss: 4.7108 - accuracy: 0.9712 - val_loss: 4.9395 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.83625\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 4.5730 - accuracy: 0.9737 - val_loss: 4.8208 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.83625\n",
            "Epoch 24/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 4.4386 - accuracy: 0.9766 - val_loss: 4.6889 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.83625\n",
            "Epoch 25/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 4.3138 - accuracy: 0.9731 - val_loss: 4.5611 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.83625\n",
            "Epoch 26/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 4.1883 - accuracy: 0.9819 - val_loss: 4.4442 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.83625\n",
            "Epoch 27/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 4.0725 - accuracy: 0.9809 - val_loss: 4.3409 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.83625\n",
            "Epoch 28/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 3.9555 - accuracy: 0.9834 - val_loss: 4.2264 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.83625\n",
            "Epoch 29/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 3.8443 - accuracy: 0.9856 - val_loss: 4.1325 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.83625\n",
            "Epoch 30/200\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 3.7443 - accuracy: 0.9856 - val_loss: 4.0315 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.83625\n",
            "Epoch 31/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 3.6388 - accuracy: 0.9881 - val_loss: 3.9331 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.83625\n",
            "Epoch 32/200\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 3.5425 - accuracy: 0.9878 - val_loss: 3.8386 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.83625\n",
            "Epoch 33/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 3.4478 - accuracy: 0.9887 - val_loss: 3.7502 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00033: val_accuracy improved from 0.83625 to 0.83750, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 34/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.3550 - accuracy: 0.9925 - val_loss: 3.6605 - val_accuracy: 0.8425\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.83750 to 0.84250, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 3.2686 - accuracy: 0.9891 - val_loss: 3.5902 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.84250\n",
            "Epoch 36/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 3.1828 - accuracy: 0.9900 - val_loss: 3.5050 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.84250\n",
            "Epoch 37/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 3.0978 - accuracy: 0.9934 - val_loss: 3.4233 - val_accuracy: 0.8438\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.84250 to 0.84375, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 38/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 3.0177 - accuracy: 0.9941 - val_loss: 3.3468 - val_accuracy: 0.8413\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.84375\n",
            "Epoch 39/200\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 2.9424 - accuracy: 0.9941 - val_loss: 3.2736 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.84375\n",
            "Epoch 40/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.8626 - accuracy: 0.9947 - val_loss: 3.2001 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.84375\n",
            "Epoch 41/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.7889 - accuracy: 0.9953 - val_loss: 3.1295 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.84375\n",
            "Epoch 42/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 2.7209 - accuracy: 0.9950 - val_loss: 3.0655 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.84375\n",
            "Epoch 43/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.6513 - accuracy: 0.9944 - val_loss: 2.9994 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.84375\n",
            "Epoch 44/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.5822 - accuracy: 0.9959 - val_loss: 2.9374 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.84375\n",
            "Epoch 45/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.5133 - accuracy: 0.9975 - val_loss: 2.8771 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.84375\n",
            "Epoch 46/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.4545 - accuracy: 0.9972 - val_loss: 2.8073 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.84375\n",
            "Epoch 47/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.3911 - accuracy: 0.9953 - val_loss: 2.7642 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.84375\n",
            "Epoch 48/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.3308 - accuracy: 0.9978 - val_loss: 2.6957 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.84375\n",
            "Epoch 49/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.2711 - accuracy: 0.9969 - val_loss: 2.6336 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.84375\n",
            "Epoch 50/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.2141 - accuracy: 0.9994 - val_loss: 2.5837 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.84375\n",
            "Epoch 51/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 2.1601 - accuracy: 0.9969 - val_loss: 2.5393 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.84375\n",
            "Epoch 52/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.1054 - accuracy: 0.9966 - val_loss: 2.4939 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.84375\n",
            "Epoch 53/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 2.0543 - accuracy: 0.9972 - val_loss: 2.4313 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.84375\n",
            "Epoch 54/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 2.0034 - accuracy: 0.9978 - val_loss: 2.3796 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.84375\n",
            "Epoch 55/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.9525 - accuracy: 0.9991 - val_loss: 2.3356 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.84375\n",
            "Epoch 56/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.9047 - accuracy: 0.9984 - val_loss: 2.2870 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.84375\n",
            "Epoch 57/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.8567 - accuracy: 0.9991 - val_loss: 2.2770 - val_accuracy: 0.8187\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.84375\n",
            "Epoch 58/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.8117 - accuracy: 1.0000 - val_loss: 2.2067 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.84375\n",
            "Epoch 59/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.7681 - accuracy: 0.9994 - val_loss: 2.1676 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.84375\n",
            "Epoch 60/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.7236 - accuracy: 0.9991 - val_loss: 2.1189 - val_accuracy: 0.8425\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.84375\n",
            "Epoch 61/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.6831 - accuracy: 0.9981 - val_loss: 2.0795 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.84375\n",
            "Epoch 62/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.6426 - accuracy: 0.9994 - val_loss: 2.0499 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.84375\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.6027 - accuracy: 0.9987 - val_loss: 2.0072 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.84375\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.5652 - accuracy: 0.9991 - val_loss: 1.9711 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.84375\n",
            "Epoch 65/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.5264 - accuracy: 0.9984 - val_loss: 1.9255 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00065: val_accuracy improved from 0.84375 to 0.84625, saving model to results/vectors/nasNet/fold_5.h5\n",
            "Epoch 66/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.4920 - accuracy: 0.9987 - val_loss: 1.8956 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.84625\n",
            "Epoch 67/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 1.4563 - accuracy: 0.9997 - val_loss: 1.8716 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.84625\n",
            "Epoch 68/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.4220 - accuracy: 0.9984 - val_loss: 1.8304 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.84625\n",
            "Epoch 69/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.3884 - accuracy: 1.0000 - val_loss: 1.7956 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.84625\n",
            "Epoch 70/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.3568 - accuracy: 0.9997 - val_loss: 1.7624 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.84625\n",
            "Epoch 71/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.3248 - accuracy: 0.9994 - val_loss: 1.7330 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.84625\n",
            "Epoch 72/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.2955 - accuracy: 0.9997 - val_loss: 1.7248 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.84625\n",
            "Epoch 73/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.2651 - accuracy: 0.9997 - val_loss: 1.6888 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.84625\n",
            "Epoch 74/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.2356 - accuracy: 0.9994 - val_loss: 1.6528 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.84625\n",
            "Epoch 75/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 1.2086 - accuracy: 0.9994 - val_loss: 1.6306 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.84625\n",
            "Epoch 76/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.1800 - accuracy: 0.9987 - val_loss: 1.6102 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.84625\n",
            "Epoch 77/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.1548 - accuracy: 0.9997 - val_loss: 1.5789 - val_accuracy: 0.8450\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.84625\n",
            "Epoch 78/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.1310 - accuracy: 0.9994 - val_loss: 1.5381 - val_accuracy: 0.8438\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.84625\n",
            "Epoch 79/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.1073 - accuracy: 0.9994 - val_loss: 1.5137 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.84625\n",
            "Epoch 80/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0825 - accuracy: 0.9994 - val_loss: 1.5007 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.84625\n",
            "Epoch 81/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0585 - accuracy: 0.9994 - val_loss: 1.4896 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.84625\n",
            "Epoch 82/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.0352 - accuracy: 0.9994 - val_loss: 1.4677 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.84625\n",
            "Epoch 83/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 1.0151 - accuracy: 0.9991 - val_loss: 1.4312 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.84625\n",
            "Epoch 84/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.9914 - accuracy: 1.0000 - val_loss: 1.4258 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.84625\n",
            "Epoch 85/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.9714 - accuracy: 0.9991 - val_loss: 1.4052 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.84625\n",
            "Epoch 86/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.9499 - accuracy: 1.0000 - val_loss: 1.3860 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.84625\n",
            "Epoch 87/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.9325 - accuracy: 0.9997 - val_loss: 1.3543 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.84625\n",
            "Epoch 88/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.9126 - accuracy: 1.0000 - val_loss: 1.3529 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.84625\n",
            "Epoch 89/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.8925 - accuracy: 0.9997 - val_loss: 1.3189 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.84625\n",
            "Epoch 90/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.8751 - accuracy: 1.0000 - val_loss: 1.3121 - val_accuracy: 0.8413\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.84625\n",
            "Epoch 91/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.8559 - accuracy: 1.0000 - val_loss: 1.2954 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.84625\n",
            "Epoch 92/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.8395 - accuracy: 1.0000 - val_loss: 1.2925 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.84625\n",
            "Epoch 93/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.8233 - accuracy: 0.9991 - val_loss: 1.2605 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.84625\n",
            "Epoch 94/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.8074 - accuracy: 0.9997 - val_loss: 1.2560 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.84625\n",
            "Epoch 95/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7891 - accuracy: 1.0000 - val_loss: 1.2325 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.84625\n",
            "Epoch 96/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7783 - accuracy: 0.9991 - val_loss: 1.2239 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.84625\n",
            "Epoch 97/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.7604 - accuracy: 0.9997 - val_loss: 1.1998 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.84625\n",
            "Epoch 98/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7456 - accuracy: 1.0000 - val_loss: 1.1843 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.84625\n",
            "Epoch 99/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7322 - accuracy: 0.9997 - val_loss: 1.1860 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.84625\n",
            "Epoch 100/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.7186 - accuracy: 1.0000 - val_loss: 1.1560 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.84625\n",
            "Epoch 101/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.7080 - accuracy: 0.9991 - val_loss: 1.1523 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.84625\n",
            "Epoch 102/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.6914 - accuracy: 1.0000 - val_loss: 1.1286 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.84625\n",
            "Epoch 103/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.6792 - accuracy: 1.0000 - val_loss: 1.1296 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.84625\n",
            "Epoch 104/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6679 - accuracy: 1.0000 - val_loss: 1.1167 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.84625\n",
            "Epoch 105/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6567 - accuracy: 0.9997 - val_loss: 1.0989 - val_accuracy: 0.8438\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.84625\n",
            "Epoch 106/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6428 - accuracy: 1.0000 - val_loss: 1.1144 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.84625\n",
            "Epoch 107/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.6348 - accuracy: 1.0000 - val_loss: 1.0722 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.84625\n",
            "Epoch 108/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.6206 - accuracy: 1.0000 - val_loss: 1.0629 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.84625\n",
            "Epoch 109/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.6101 - accuracy: 0.9991 - val_loss: 1.0511 - val_accuracy: 0.8413\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.84625\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.6001 - accuracy: 1.0000 - val_loss: 1.0595 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.84625\n",
            "Epoch 111/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.5903 - accuracy: 0.9997 - val_loss: 1.0454 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.84625\n",
            "Epoch 112/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.5803 - accuracy: 1.0000 - val_loss: 1.0270 - val_accuracy: 0.8450\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.84625\n",
            "Epoch 113/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.5706 - accuracy: 1.0000 - val_loss: 1.0537 - val_accuracy: 0.8150\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.84625\n",
            "Epoch 114/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.5602 - accuracy: 1.0000 - val_loss: 1.0181 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.84625\n",
            "Epoch 115/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.5495 - accuracy: 0.9994 - val_loss: 1.0151 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.84625\n",
            "Epoch 116/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.5414 - accuracy: 1.0000 - val_loss: 0.9978 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.84625\n",
            "Epoch 117/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.5329 - accuracy: 1.0000 - val_loss: 0.9824 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.84625\n",
            "Epoch 118/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.5255 - accuracy: 1.0000 - val_loss: 0.9781 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.84625\n",
            "Epoch 119/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.5176 - accuracy: 0.9994 - val_loss: 0.9760 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.84625\n",
            "Epoch 120/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.5082 - accuracy: 1.0000 - val_loss: 0.9815 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.84625\n",
            "Epoch 121/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4993 - accuracy: 1.0000 - val_loss: 0.9674 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.84625\n",
            "Epoch 122/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4923 - accuracy: 1.0000 - val_loss: 0.9475 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.84625\n",
            "Epoch 123/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4864 - accuracy: 1.0000 - val_loss: 0.9544 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.84625\n",
            "Epoch 124/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4765 - accuracy: 1.0000 - val_loss: 0.9466 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.84625\n",
            "Epoch 125/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4704 - accuracy: 0.9997 - val_loss: 0.9162 - val_accuracy: 0.8413\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.84625\n",
            "Epoch 126/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4631 - accuracy: 1.0000 - val_loss: 0.9176 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.84625\n",
            "Epoch 127/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 0.4565 - accuracy: 1.0000 - val_loss: 0.9239 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.84625\n",
            "Epoch 128/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4497 - accuracy: 0.9994 - val_loss: 0.9029 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.84625\n",
            "Epoch 129/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4430 - accuracy: 1.0000 - val_loss: 0.9099 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.84625\n",
            "Epoch 130/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4369 - accuracy: 1.0000 - val_loss: 0.9217 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.84625\n",
            "Epoch 131/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.4300 - accuracy: 1.0000 - val_loss: 0.8865 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.84625\n",
            "Epoch 132/200\n",
            "100/100 [==============================] - 6s 63ms/step - loss: 0.4237 - accuracy: 1.0000 - val_loss: 0.8827 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.84625\n",
            "Epoch 133/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4191 - accuracy: 1.0000 - val_loss: 0.8731 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.84625\n",
            "Epoch 134/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4136 - accuracy: 1.0000 - val_loss: 0.8731 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.84625\n",
            "Epoch 135/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4073 - accuracy: 1.0000 - val_loss: 0.8676 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.84625\n",
            "Epoch 136/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.4020 - accuracy: 1.0000 - val_loss: 0.8615 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.84625\n",
            "Epoch 137/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3957 - accuracy: 0.9997 - val_loss: 0.8582 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.84625\n",
            "Epoch 138/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3904 - accuracy: 1.0000 - val_loss: 0.8610 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.84625\n",
            "Epoch 139/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3847 - accuracy: 1.0000 - val_loss: 0.8342 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.84625\n",
            "Epoch 140/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3806 - accuracy: 1.0000 - val_loss: 0.8576 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.84625\n",
            "Epoch 141/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3753 - accuracy: 1.0000 - val_loss: 0.8488 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.84625\n",
            "Epoch 142/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3707 - accuracy: 1.0000 - val_loss: 0.8388 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.84625\n",
            "Epoch 143/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3644 - accuracy: 1.0000 - val_loss: 0.8259 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.84625\n",
            "Epoch 144/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3605 - accuracy: 0.9997 - val_loss: 0.8410 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.84625\n",
            "Epoch 145/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3566 - accuracy: 1.0000 - val_loss: 0.8255 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.84625\n",
            "Epoch 146/200\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 0.3509 - accuracy: 1.0000 - val_loss: 0.8163 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.84625\n",
            "Epoch 147/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3464 - accuracy: 1.0000 - val_loss: 0.8303 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.84625\n",
            "Epoch 148/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3441 - accuracy: 1.0000 - val_loss: 0.8105 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.84625\n",
            "Epoch 149/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3388 - accuracy: 1.0000 - val_loss: 0.8208 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.84625\n",
            "Epoch 150/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3356 - accuracy: 0.9994 - val_loss: 0.7985 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.84625\n",
            "Epoch 151/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3313 - accuracy: 1.0000 - val_loss: 0.8028 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.84625\n",
            "Epoch 152/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3265 - accuracy: 1.0000 - val_loss: 0.8648 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.84625\n",
            "Epoch 153/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3229 - accuracy: 1.0000 - val_loss: 0.8092 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.84625\n",
            "Epoch 154/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3188 - accuracy: 1.0000 - val_loss: 0.8018 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.84625\n",
            "Epoch 155/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.7922 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.84625\n",
            "Epoch 156/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3121 - accuracy: 1.0000 - val_loss: 0.7800 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.84625\n",
            "Epoch 157/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.3074 - accuracy: 0.9997 - val_loss: 0.8080 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.84625\n",
            "Epoch 158/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3040 - accuracy: 1.0000 - val_loss: 0.7884 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.84625\n",
            "Epoch 159/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.3013 - accuracy: 1.0000 - val_loss: 0.7881 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.84625\n",
            "Epoch 160/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2982 - accuracy: 0.9997 - val_loss: 0.7633 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.84625\n",
            "Epoch 161/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2950 - accuracy: 0.9997 - val_loss: 0.7599 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.84625\n",
            "Epoch 162/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2922 - accuracy: 1.0000 - val_loss: 0.7860 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.84625\n",
            "Epoch 163/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2879 - accuracy: 1.0000 - val_loss: 0.7545 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.84625\n",
            "Epoch 164/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2847 - accuracy: 0.9997 - val_loss: 0.7590 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.84625\n",
            "Epoch 165/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2814 - accuracy: 0.9997 - val_loss: 0.7578 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.84625\n",
            "Epoch 166/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2795 - accuracy: 1.0000 - val_loss: 0.7693 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.84625\n",
            "Epoch 167/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2754 - accuracy: 1.0000 - val_loss: 0.7540 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.84625\n",
            "Epoch 168/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2731 - accuracy: 1.0000 - val_loss: 0.7548 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.84625\n",
            "Epoch 169/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2695 - accuracy: 1.0000 - val_loss: 0.7453 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.84625\n",
            "Epoch 170/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2668 - accuracy: 1.0000 - val_loss: 0.7460 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.84625\n",
            "Epoch 171/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2645 - accuracy: 1.0000 - val_loss: 0.7539 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.84625\n",
            "Epoch 172/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2603 - accuracy: 1.0000 - val_loss: 0.7577 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.84625\n",
            "Epoch 173/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2584 - accuracy: 1.0000 - val_loss: 0.7352 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.84625\n",
            "Epoch 174/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2550 - accuracy: 0.9997 - val_loss: 0.7453 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.84625\n",
            "Epoch 175/200\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 0.2533 - accuracy: 1.0000 - val_loss: 0.7219 - val_accuracy: 0.8388\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.84625\n",
            "Epoch 176/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2516 - accuracy: 1.0000 - val_loss: 0.7187 - val_accuracy: 0.8338\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.84625\n",
            "Epoch 177/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2484 - accuracy: 1.0000 - val_loss: 0.7439 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.84625\n",
            "Epoch 178/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2476 - accuracy: 1.0000 - val_loss: 0.7091 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.84625\n",
            "Epoch 179/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2455 - accuracy: 1.0000 - val_loss: 0.7629 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.84625\n",
            "Epoch 180/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2439 - accuracy: 1.0000 - val_loss: 0.7335 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.84625\n",
            "Epoch 181/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2397 - accuracy: 1.0000 - val_loss: 0.7271 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.84625\n",
            "Epoch 182/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2376 - accuracy: 0.9997 - val_loss: 0.7250 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.84625\n",
            "Epoch 183/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2341 - accuracy: 1.0000 - val_loss: 0.7182 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.84625\n",
            "Epoch 184/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2322 - accuracy: 1.0000 - val_loss: 0.7156 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.84625\n",
            "Epoch 185/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2308 - accuracy: 1.0000 - val_loss: 0.7348 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.84625\n",
            "Epoch 186/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2279 - accuracy: 1.0000 - val_loss: 0.7085 - val_accuracy: 0.8413\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.84625\n",
            "Epoch 187/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2264 - accuracy: 1.0000 - val_loss: 0.7073 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.84625\n",
            "Epoch 188/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2237 - accuracy: 1.0000 - val_loss: 0.6961 - val_accuracy: 0.8312\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.84625\n",
            "Epoch 189/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2217 - accuracy: 0.9997 - val_loss: 0.6940 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.84625\n",
            "Epoch 190/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2209 - accuracy: 1.0000 - val_loss: 0.7074 - val_accuracy: 0.8363\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.84625\n",
            "Epoch 191/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2186 - accuracy: 1.0000 - val_loss: 0.6985 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.84625\n",
            "Epoch 192/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2174 - accuracy: 1.0000 - val_loss: 0.6992 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.84625\n",
            "Epoch 193/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2176 - accuracy: 1.0000 - val_loss: 0.7059 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.84625\n",
            "Epoch 194/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2133 - accuracy: 1.0000 - val_loss: 0.6838 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.84625\n",
            "Epoch 195/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2110 - accuracy: 1.0000 - val_loss: 0.6957 - val_accuracy: 0.8287\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.84625\n",
            "Epoch 196/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2108 - accuracy: 1.0000 - val_loss: 0.6803 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.84625\n",
            "Epoch 197/200\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.2070 - accuracy: 1.0000 - val_loss: 0.6970 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.84625\n",
            "Epoch 198/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2049 - accuracy: 1.0000 - val_loss: 0.6892 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.84625\n",
            "Epoch 199/200\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.2033 - accuracy: 1.0000 - val_loss: 0.6805 - val_accuracy: 0.8237\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.84625\n",
            "Epoch 200/200\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.2021 - accuracy: 1.0000 - val_loss: 0.6897 - val_accuracy: 0.8275\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.84625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMJqLBMPzrfK"
      },
      "source": [
        "## save folds_y folds_y_pred and histories\n",
        "## (folds_y folds_y_pred) to calcutate metrics (accuracy, recall ...)\n",
        "## histories_ for ploting accuracy and loss curves\n",
        "pg.save_folds_vectors(folds_y,  tc_algo_folder, name=\"folds_y_vf.npy\")\n",
        "pg.save_folds_vectors(folds_y_pred,  tc_algo_folder, name=\"folds_y_pred_vf.npy\")\n",
        "pg.save_folds_vectors(histories,  tc_algo_folder, name=\"histories__vf.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DbpXBJhlWoTS",
        "outputId": "2458d0f3-dd9e-47cd-c9bd-619096fa1adf"
      },
      "source": [
        "# ploting folds accuracy and loss curves\n",
        "for i, histo in enumerate(histories):\n",
        " pg.plot_loss_acc(histo, epochs, \"Fold \"+str(i+1)+\"(\"+tc_algo_folder+\")\")\n",
        " plt.savefig('/content/drive/MyDrive/Breast_Cancer/results/plots/'+tc_algo_folder+'/plt_fold'+str(i+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wqKEggJzqmO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}